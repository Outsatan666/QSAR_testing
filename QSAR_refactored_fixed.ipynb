{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# QSAR pipeline: reproducible exploratory SAR + ranking QSAR within applicability domain\n\nЭтот ноутбук строит воспроизводимый конвейер с честным разделением данных: `train_lit` (строки 0–89) и `experimental_holdout` (строки 90+). Если внешняя предсказательная сила слабая, вывод остаётся валидным за счёт статистического SAR-блока, y-randomization и Applicability Domain."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_PATH = ROOT / \"potok.csv\"\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "FIG_DIR = ARTIFACTS / \"figures\"\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = ARTIFACTS / \"run.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(\"qsar\")\n",
    "logger.info(\"Run started\")\n",
    "\n",
    "config = {\n",
    "    \"seed\": SEED,\n",
    "    \"data_path\": str(DATA_PATH),\n",
    "    \"split_strategy\": \"fixed rows: train_df=0..89, exp_df=90+; GroupKFold by Murcko scaffold\",\n",
    "    \"duplicate_policy\": \"median\",\n",
    "    \"low_mic_quantile\": 0.25,\n",
    "    \"cv_n_splits\": 5,\n",
    "    \"fingerprint\": {\"radius\": 2, \"nBits\": 2048},\n",
    "    \"fragment_min_train_presence\": 5,\n",
    "    \"fragment_min_lowmic_presence\": 3,\n",
    "    \"yrandomization_n\": 50,\n",
    "    \"or_bootstrap_n\": 300,\n",
    "}\n",
    "(ARTIFACTS / \"run_config.json\").write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "logger.info(\"Saved initial run_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency check (safe stop if RDKit is unavailable)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    from sklearn.base import clone\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import (\n",
    "        balanced_accuracy_score,\n",
    "        average_precision_score,\n",
    "        precision_recall_curve,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "    )\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem\n",
    "    from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "    from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "except Exception as e:\n",
    "    msg = (\n",
    "        \"RDKit и/или научный стек недоступны. Нужна локальная среда с RDKit, numpy, pandas, scipy, scikit-learn, matplotlib. \"\n",
    "        \"В ноутбуке pip-установка не выполняется по правилам воспроизводимости.\"\n",
    "    )\n",
    "    logger.error(msg)\n",
    "    logger.error(f\"Import error: {e}\")\n",
    "    raise SystemExit(msg)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger.info(\"Dependencies imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data loading and fixed split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    logger.error(f\"Missing data file: {DATA_PATH}\")\n",
    "    raise FileNotFoundError(DATA_PATH)\n",
    "\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "logger.info(f\"Raw rows: {len(raw)}\")\n",
    "\n",
    "smiles_col = next((c for c in raw.columns if c.lower() in [\"smiles\", \"smile\", \"canonical_smiles\"]), None)\n",
    "activity_col = next((c for c in raw.columns if c.lower() in [\"mic\", \"activity\", \"y\", \"target\", \"pmic\"]), None)\n",
    "if smiles_col is None or activity_col is None:\n",
    "    raise ValueError(\"Expected columns for SMILES and activity (MIC)\")\n",
    "\n",
    "df = raw[[smiles_col, activity_col]].copy().rename(columns={smiles_col: \"smiles\", activity_col: \"MIC\"})\n",
    "df[\"row_id\"] = np.arange(len(df))\n",
    "df[\"MIC\"] = pd.to_numeric(df[\"MIC\"], errors=\"coerce\")\n",
    "if df[\"MIC\"].isna().any():\n",
    "    raise ValueError(\"MIC contains non-numeric values\")\n",
    "if not (df[\"MIC\"] > 0).all():\n",
    "    raise ValueError(\"MIC must be > 0 for pMIC = -log10(MIC)\")\n",
    "\n",
    "df[\"pMIC\"] = -np.log10(df[\"MIC\"].astype(float))\n",
    "\n",
    "train_lit = df[df[\"row_id\"] < 90].copy()\n",
    "experimental_holdout = df[df[\"row_id\"] >= 90].copy()\n",
    "assert len(train_lit) == min(90, len(df)), \"train split mismatch\"\n",
    "assert set(train_lit[\"row_id\"]).isdisjoint(set(experimental_holdout[\"row_id\"])), \"split leakage\"\n",
    "logger.info(f\"Split sizes | train_df={len(train_lit)} | exp_df={len(experimental_holdout)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data curation (mandatory)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc = rdMolStandardize.LargestFragmentChooser()\n",
    "\n",
    "def standardize_smiles(smi):\n",
    "    mol = Chem.MolFromSmiles(str(smi))\n",
    "    if mol is None:\n",
    "        return None, \"invalid_smiles\"\n",
    "    largest = lfc.choose(mol)\n",
    "    if largest is None:\n",
    "        return None, \"no_largest_fragment\"\n",
    "    can = Chem.MolToSmiles(largest, canonical=True)\n",
    "    return can, \"ok\"\n",
    "\n",
    "\n",
    "def curate_block(block, block_name, duplicate_policy=\"median\"):\n",
    "    report_rows = []\n",
    "    rows = []\n",
    "    for _, r in block.iterrows():\n",
    "        std_smi, status = standardize_smiles(r[\"smiles\"])\n",
    "        action = \"keep\" if status == \"ok\" else \"drop\"\n",
    "        report_rows.append({\n",
    "            \"block\": block_name,\n",
    "            \"row_id\": int(r[\"row_id\"]),\n",
    "            \"original_smiles\": r[\"smiles\"],\n",
    "            \"standardized_smiles\": std_smi,\n",
    "            \"MIC\": float(r[\"MIC\"]),\n",
    "            \"pMIC\": float(r[\"pMIC\"]),\n",
    "            \"action\": action,\n",
    "            \"reason\": status,\n",
    "        })\n",
    "        if status == \"ok\":\n",
    "            rows.append({\"row_id\": int(r[\"row_id\"]), \"smiles\": std_smi, \"MIC\": float(r[\"MIC\"])})\n",
    "\n",
    "    clean = pd.DataFrame(rows)\n",
    "    if clean.empty:\n",
    "        return clean, pd.DataFrame(report_rows)\n",
    "\n",
    "    grp = clean.groupby(\"smiles\")[\"MIC\"]\n",
    "    agg_rows = []\n",
    "    for smi, vals in grp:\n",
    "        vals = vals.values.astype(float)\n",
    "        if len(vals) == 1:\n",
    "            mic = float(vals[0])\n",
    "            agg_rows.append({\"smiles\": smi, \"MIC\": mic, \"pMIC\": float(-np.log10(mic))})\n",
    "            continue\n",
    "        conflict = np.ptp(vals) > 1e-12\n",
    "        if duplicate_policy == \"median\":\n",
    "            mic = float(np.median(vals))\n",
    "            reason = \"duplicate_median\"\n",
    "            agg_rows.append({\"smiles\": smi, \"MIC\": mic, \"pMIC\": float(-np.log10(mic))})\n",
    "        elif duplicate_policy == \"mean\":\n",
    "            mic = float(np.mean(vals))\n",
    "            reason = \"duplicate_mean\"\n",
    "            agg_rows.append({\"smiles\": smi, \"MIC\": mic, \"pMIC\": float(-np.log10(mic))})\n",
    "        elif duplicate_policy == \"drop_conflicts\":\n",
    "            if conflict:\n",
    "                reason = \"duplicate_conflict_drop\"\n",
    "            else:\n",
    "                mic = float(vals[0])\n",
    "                reason = \"duplicate_identical_keep\"\n",
    "                agg_rows.append({\"smiles\": smi, \"MIC\": mic, \"pMIC\": float(-np.log10(mic))})\n",
    "        else:\n",
    "            raise ValueError(\"duplicate_policy must be median|mean|drop_conflicts\")\n",
    "\n",
    "        idxs = clean.loc[clean[\"smiles\"] == smi, \"row_id\"].tolist()\n",
    "        for rid in idxs:\n",
    "            report_rows.append({\n",
    "                \"block\": block_name,\n",
    "                \"row_id\": int(rid),\n",
    "                \"original_smiles\": None,\n",
    "                \"standardized_smiles\": smi,\n",
    "                \"MIC\": None,\n",
    "                \"pMIC\": None,\n",
    "                \"action\": \"duplicate_resolution\",\n",
    "                \"reason\": reason,\n",
    "            })\n",
    "\n",
    "    curated = pd.DataFrame(agg_rows).drop_duplicates(\"smiles\").reset_index(drop=True)\n",
    "    curated[\"block\"] = block_name\n",
    "    return curated, pd.DataFrame(report_rows)\n",
    "\n",
    "train_cur, rep_train = curate_block(train_lit, \"train_lit\", duplicate_policy=config[\"duplicate_policy\"])\n",
    "exp_cur, rep_exp = curate_block(experimental_holdout, \"experimental_holdout\", duplicate_policy=config[\"duplicate_policy\"])\n",
    "\n",
    "# remove cross-split leakage: any SMILES that appears in train and experimental is removed from experimental\n",
    "shared_smiles = set(train_cur[\"smiles\"]).intersection(set(exp_cur[\"smiles\"]))\n",
    "removed_cross_split = int(len(shared_smiles))\n",
    "if removed_cross_split:\n",
    "    for smi in sorted(shared_smiles):\n",
    "        rep_exp = pd.concat(\n",
    "            [\n",
    "                rep_exp,\n",
    "                pd.DataFrame([\n",
    "                    {\n",
    "                        \"block\": \"experimental_holdout\",\n",
    "                        \"row_id\": None,\n",
    "                        \"original_smiles\": None,\n",
    "                        \"standardized_smiles\": smi,\n",
    "                        \"MIC\": None,\n",
    "                        \"pMIC\": None,\n",
    "                        \"action\": \"drop\",\n",
    "                        \"reason\": \"cross_split_duplicate_removed\",\n",
    "                    }\n",
    "                ]),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    exp_cur = exp_cur.loc[~exp_cur[\"smiles\"].isin(shared_smiles)].reset_index(drop=True)\n",
    "\n",
    "curation_report = pd.concat([rep_train, rep_exp], ignore_index=True)\n",
    "curation_report.to_csv(ARTIFACTS / \"curation_report.csv\", index=False)\n",
    "\n",
    "curation_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"train_curated_n\": int(len(train_cur)),\n",
    "        \"experimental_curated_n\": int(len(exp_cur)),\n",
    "        \"cross_split_duplicates_removed_from_experimental\": removed_cross_split,\n",
    "    }\n",
    "])\n",
    "curation_summary.to_csv(ARTIFACTS / \"curation_summary.csv\", index=False)\n",
    "\n",
    "assert train_cur[\"smiles\"].isna().sum() == 0, \"NaN in curated smiles\"\n",
    "assert {\"MIC\", \"pMIC\"}.issubset(train_cur.columns), \"train_cur must have MIC and pMIC\"\n",
    "assert np.isfinite(train_cur[\"MIC\"]).all() and np.isfinite(train_cur[\"pMIC\"]).all(), \"Invalid activity values\"\n",
    "assert len(set(train_cur[\"smiles\"]).intersection(set(exp_cur[\"smiles\"]))) == 0, \"cross-split leakage after curation\"\n",
    "logger.info(\n",
    "    f\"After curation | train_lit={len(train_cur)} | experimental_holdout={len(exp_cur)} | removed_cross_split={removed_cross_split}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature engineering, CV, Q², external tests, SAR statistics, y-randomization, AD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse QSAR (activity-conditioned SAR): descriptor stats, scaffold-aware classification,\n",
    "# fragment enrichment, rule extraction, experimental holdout checks, y-randomization.\n",
    "\n",
    "def murcko_scaffold(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return \"INVALID\"\n",
    "    scaf = MurckoScaffold.MurckoScaffoldSmiles(mol=mol)\n",
    "    return scaf if scaf else \"ACYCLIC\"\n",
    "\n",
    "\n",
    "def desc_dict(mol):\n",
    "    return {\n",
    "        \"MolWt\": Descriptors.MolWt(mol),\n",
    "        \"MolLogP\": Descriptors.MolLogP(mol),\n",
    "        \"TPSA\": Descriptors.TPSA(mol),\n",
    "        \"NumHDonors\": Descriptors.NumHDonors(mol),\n",
    "        \"NumHAcceptors\": Descriptors.NumHAcceptors(mol),\n",
    "        \"NumRotatableBonds\": Descriptors.NumRotatableBonds(mol),\n",
    "        \"RingCount\": Descriptors.RingCount(mol),\n",
    "        \"HeavyAtomCount\": Descriptors.HeavyAtomCount(mol),\n",
    "    }\n",
    "\n",
    "\n",
    "def featurize(data):\n",
    "    rows = []\n",
    "    fps = []\n",
    "    for _, r in data.iterrows():\n",
    "        mol = Chem.MolFromSmiles(r[\"smiles\"])\n",
    "        if mol is None:\n",
    "            continue\n",
    "        d = desc_dict(mol)\n",
    "        d.update({\"smiles\": r[\"smiles\"], \"MIC\": r[\"MIC\"], \"pMIC\": r[\"pMIC\"], \"scaffold\": murcko_scaffold(r[\"smiles\"])})\n",
    "        rows.append(d)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(\n",
    "            mol,\n",
    "            radius=config[\"fingerprint\"][\"radius\"],\n",
    "            nBits=config[\"fingerprint\"][\"nBits\"],\n",
    "        )\n",
    "        fps.append(np.array(fp, dtype=int))\n",
    "    desc = pd.DataFrame(rows)\n",
    "    fp_df = pd.DataFrame(fps, columns=[f\"FP_{i}\" for i in range(config[\"fingerprint\"][\"nBits\"])]) if len(fps) else pd.DataFrame()\n",
    "    return desc.reset_index(drop=True), fp_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def bh_fdr(pvals):\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    n = len(pvals)\n",
    "    order = np.argsort(pvals)\n",
    "    ranked = pvals[order]\n",
    "    q = ranked * n / (np.arange(1, n + 1))\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    q = np.clip(q, 0, 1)\n",
    "    out = np.empty_like(q)\n",
    "    out[order] = q\n",
    "    return out\n",
    "\n",
    "\n",
    "def cliffs_delta(x, y):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    gt = np.sum(x[:, None] > y[None, :])\n",
    "    lt = np.sum(x[:, None] < y[None, :])\n",
    "    return (gt - lt) / (len(x) * len(y))\n",
    "\n",
    "\n",
    "train_desc, train_fp = featurize(train_cur)\n",
    "exp_desc, exp_fp = featurize(exp_cur) if len(exp_cur) else (pd.DataFrame(), pd.DataFrame())\n",
    "\n",
    "assert {\"MIC\", \"pMIC\"}.issubset(train_desc.columns), \"train_desc must have MIC and pMIC\"\n",
    "if len(exp_desc):\n",
    "    assert {\"MIC\", \"pMIC\"}.issubset(exp_desc.columns), \"exp_desc must have MIC and pMIC\"\n",
    "\n",
    "assert len(set(train_cur[\"smiles\"]).intersection(set(exp_cur[\"smiles\"]))) == 0, \"cross-split leakage persists\"\n",
    "\n",
    "desc_cols = [c for c in train_desc.columns if c not in [\"smiles\", \"MIC\", \"pMIC\", \"scaffold\"]]\n",
    "\n",
    "# low_MIC label from train only\n",
    "low_mic_threshold = float(np.quantile(train_desc[\"MIC\"], config[\"low_mic_quantile\"]))\n",
    "train_desc[\"low_MIC\"] = (train_desc[\"MIC\"] <= low_mic_threshold).astype(int)\n",
    "if len(exp_desc):\n",
    "    exp_desc[\"low_MIC\"] = (exp_desc[\"MIC\"] <= low_mic_threshold).astype(int)\n",
    "\n",
    "config[\"low_MIC_threshold\"] = low_mic_threshold\n",
    "(ARTIFACTS / \"run_config.json\").write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "X_desc = train_desc[desc_cols].copy()\n",
    "y_cls = train_desc[\"low_MIC\"].values\n",
    "groups = train_desc[\"scaffold\"].values\n",
    "assert np.array_equal(np.sort(np.unique(y_cls)), np.array([0, 1])), \"Need both classes for classification\"\n",
    "\n",
    "# A) Descriptor stats (train only)\n",
    "mi_vals = mutual_info_classif(X_desc, y_cls, random_state=SEED, discrete_features=False)\n",
    "mi_map = dict(zip(desc_cols, mi_vals))\n",
    "\n",
    "rows = []\n",
    "for c in desc_cols:\n",
    "    sp = stats.spearmanr(train_desc[c], train_desc[\"pMIC\"], nan_policy=\"omit\")\n",
    "    kd = stats.kendalltau(train_desc[c], train_desc[\"pMIC\"], nan_policy=\"omit\")\n",
    "    rows.append({\n",
    "        \"descriptor\": c,\n",
    "        \"spearman_rho\": sp.correlation,\n",
    "        \"spearman_p\": sp.pvalue,\n",
    "        \"kendall_tau\": kd.correlation,\n",
    "        \"kendall_p\": kd.pvalue,\n",
    "        \"mutual_information_low_MIC\": float(mi_map.get(c, np.nan)),\n",
    "    })\n",
    "cor_df = pd.DataFrame(rows)\n",
    "cor_df[\"spearman_q\"] = bh_fdr(cor_df[\"spearman_p\"].values)\n",
    "cor_df[\"kendall_q\"] = bh_fdr(cor_df[\"kendall_p\"].values)\n",
    "cor_df = cor_df.sort_values(\"spearman_q\")\n",
    "cor_df.to_csv(ARTIFACTS / \"descriptor_correlations.csv\", index=False)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "uni_rows = []\n",
    "for c in desc_cols:\n",
    "    low_vals = train_desc.loc[train_desc[\"low_MIC\"] == 1, c].values\n",
    "    high_vals = train_desc.loc[train_desc[\"low_MIC\"] == 0, c].values\n",
    "    mw = stats.mannwhitneyu(low_vals, high_vals, alternative=\"two-sided\")\n",
    "    delta = cliffs_delta(low_vals, high_vals)\n",
    "\n",
    "    scores = train_desc[c].values\n",
    "    fpr, tpr, thresholds = roc_curve(y_cls, scores)\n",
    "    youden = tpr - fpr\n",
    "    best_idx = int(np.argmax(youden))\n",
    "\n",
    "    x = train_desc[[c]].values.astype(float)\n",
    "    x = (x - x.mean(axis=0)) / (x.std(axis=0) + 1e-12)\n",
    "    try:\n",
    "        lr = LogisticRegression(solver=\"liblinear\", random_state=SEED)\n",
    "        lr.fit(x, y_cls)\n",
    "        beta = float(lr.coef_[0][0])\n",
    "        or_point = float(np.exp(beta))\n",
    "\n",
    "        boots = []\n",
    "        for _ in range(int(config[\"or_bootstrap_n\"])):\n",
    "            idx = rng.choice(len(y_cls), size=len(y_cls), replace=True)\n",
    "            yb = y_cls[idx]\n",
    "            if len(np.unique(yb)) < 2:\n",
    "                continue\n",
    "            xb = x[idx]\n",
    "            try:\n",
    "                blr = LogisticRegression(solver=\"liblinear\", random_state=SEED)\n",
    "                blr.fit(xb, yb)\n",
    "                boots.append(float(np.exp(blr.coef_[0][0])))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if len(boots) >= 20:\n",
    "            or_low, or_high = np.percentile(boots, [2.5, 97.5])\n",
    "        else:\n",
    "            or_low, or_high = np.nan, np.nan\n",
    "    except Exception:\n",
    "        or_point, or_low, or_high = np.nan, np.nan, np.nan\n",
    "\n",
    "    uni_rows.append({\n",
    "        \"descriptor\": c,\n",
    "        \"mannwhitney_u\": mw.statistic,\n",
    "        \"mannwhitney_p\": mw.pvalue,\n",
    "        \"cliffs_delta\": delta,\n",
    "        \"roc_auc\": roc_auc_score(y_cls, scores),\n",
    "        \"youden_index\": float(youden[best_idx]),\n",
    "        \"youden_threshold\": float(thresholds[best_idx]),\n",
    "        \"window_operator\": \"<=\" if delta < 0 else \">=\",\n",
    "        \"odds_ratio\": or_point,\n",
    "        \"odds_ratio_ci_low\": or_low,\n",
    "        \"odds_ratio_ci_high\": or_high,\n",
    "    })\n",
    "\n",
    "uni_df = pd.DataFrame(uni_rows)\n",
    "uni_df[\"mannwhitney_q\"] = bh_fdr(uni_df[\"mannwhitney_p\"].values)\n",
    "uni_df = uni_df.sort_values(\"mannwhitney_q\")\n",
    "uni_df.to_csv(ARTIFACTS / \"univariate_statistics.csv\", index=False)\n",
    "\n",
    "important_windows = uni_df.loc[uni_df[\"mannwhitney_q\"] <= 0.1].copy()\n",
    "if important_windows.empty:\n",
    "    important_windows = uni_df.head(min(5, len(uni_df))).copy()\n",
    "descriptor_windows = important_windows[[\"descriptor\", \"youden_threshold\", \"window_operator\", \"roc_auc\", \"mannwhitney_q\"]].rename(\n",
    "    columns={\n",
    "        \"youden_threshold\": \"threshold\",\n",
    "        \"window_operator\": \"rule\",\n",
    "        \"mannwhitney_q\": \"q_value\",\n",
    "    }\n",
    ")\n",
    "descriptor_windows.to_csv(ARTIFACTS / \"descriptor_windows.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=cor_df, x=\"descriptor\", y=\"spearman_rho\", color=\"#4477aa\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Spearman correlation with pMIC (train)\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"descriptor_spearman.png\", dpi=220); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=uni_df, x=\"descriptor\", y=\"roc_auc\", color=\"#66aa55\")\n",
    "plt.axhline(0.5, ls=\"--\", color=\"k\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Univariate descriptor ROC-AUC for low_MIC\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"descriptor_univariate_auc.png\", dpi=220); plt.close()\n",
    "\n",
    "# B) Scaffold-aware classification (train only)\n",
    "models = {\n",
    "    \"LogReg_L2\": Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", max_iter=5000, random_state=SEED)),\n",
    "    ]),\n",
    "    \"LogReg_L1\": Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=5000, random_state=SEED)),\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=SEED, n_jobs=-1, class_weight=\"balanced\"),\n",
    "}\n",
    "\n",
    "n_splits = min(config[\"cv_n_splits\"], len(np.unique(groups)))\n",
    "cv = GroupKFold(n_splits=n_splits)\n",
    "clf_rows = []\n",
    "roc_curves = {}\n",
    "pr_curves = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    fold_metrics = []\n",
    "    oof_proba = np.zeros(len(train_desc), dtype=float)\n",
    "\n",
    "    for fold, (tr, te) in enumerate(cv.split(X_desc, y_cls, groups), start=1):\n",
    "        Xtr, Xte = X_desc.iloc[tr], X_desc.iloc[te]\n",
    "        ytr, yte = y_cls[tr], y_cls[te]\n",
    "        m = clone(model)\n",
    "        m.fit(Xtr, ytr)\n",
    "        prob = m.predict_proba(Xte)[:, 1]\n",
    "        pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "        oof_proba[te] = prob\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold,\n",
    "            \"roc_auc\": roc_auc_score(yte, prob),\n",
    "            \"pr_auc\": average_precision_score(yte, prob),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(yte, pred),\n",
    "        })\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "    clf_rows.append({\n",
    "        \"model\": name,\n",
    "        \"roc_auc_mean\": fold_df[\"roc_auc\"].mean(),\n",
    "        \"roc_auc_std\": fold_df[\"roc_auc\"].std(ddof=1),\n",
    "        \"pr_auc_mean\": fold_df[\"pr_auc\"].mean(),\n",
    "        \"pr_auc_std\": fold_df[\"pr_auc\"].std(ddof=1),\n",
    "        \"balanced_accuracy_mean\": fold_df[\"balanced_accuracy\"].mean(),\n",
    "        \"balanced_accuracy_std\": fold_df[\"balanced_accuracy\"].std(ddof=1),\n",
    "    })\n",
    "    roc_curves[name] = roc_curve(y_cls, oof_proba)\n",
    "    pr_curves[name] = precision_recall_curve(y_cls, oof_proba)\n",
    "\n",
    "classification_metrics = pd.DataFrame(clf_rows).sort_values(\"roc_auc_mean\", ascending=False)\n",
    "classification_metrics.to_csv(ARTIFACTS / \"classification_metrics.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name, (fpr, tpr, _) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"OOF ROC curves (GroupKFold by scaffold)\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"classification_roc.png\", dpi=220); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name, (prec, rec, _) in pr_curves.items():\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"OOF PR curves (GroupKFold by scaffold)\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"classification_pr.png\", dpi=220); plt.close()\n",
    "\n",
    "# C) Fragment enrichment on train\n",
    "frag_rows = []\n",
    "for col in train_fp.columns:\n",
    "    bit = train_fp[col].values.astype(int)\n",
    "    present_train = int(bit.sum())\n",
    "    present_low = int(bit[train_desc[\"low_MIC\"].values == 1].sum())\n",
    "    if present_train < config[\"fragment_min_train_presence\"] or present_low < config[\"fragment_min_lowmic_presence\"]:\n",
    "        continue\n",
    "\n",
    "    a = int(np.sum((bit == 1) & (y_cls == 1)))\n",
    "    b = int(np.sum((bit == 1) & (y_cls == 0)))\n",
    "    c = int(np.sum((bit == 0) & (y_cls == 1)))\n",
    "    d = int(np.sum((bit == 0) & (y_cls == 0)))\n",
    "    _, p = stats.fisher_exact([[a, b], [c, d]], alternative=\"greater\")\n",
    "\n",
    "    odds_ratio = ((a + 0.5) * (d + 0.5)) / ((b + 0.5) * (c + 0.5))\n",
    "    enrichment_factor = (a / max(1, a + b)) / (np.sum(y_cls == 1) / len(y_cls))\n",
    "\n",
    "    frag_rows.append({\n",
    "        \"fragment\": col,\n",
    "        \"a_low_present\": a,\n",
    "        \"b_nonlow_present\": b,\n",
    "        \"c_low_absent\": c,\n",
    "        \"d_nonlow_absent\": d,\n",
    "        \"odds_ratio\": odds_ratio,\n",
    "        \"enrichment_factor\": enrichment_factor,\n",
    "        \"fisher_p\": p,\n",
    "    })\n",
    "\n",
    "frag_df = pd.DataFrame(frag_rows)\n",
    "if len(frag_df):\n",
    "    frag_df[\"fisher_q\"] = bh_fdr(frag_df[\"fisher_p\"].values)\n",
    "    frag_df = frag_df.sort_values([\"fisher_q\", \"odds_ratio\"], ascending=[True, False])\n",
    "else:\n",
    "    frag_df = pd.DataFrame(columns=[\"fragment\", \"fisher_p\", \"fisher_q\", \"odds_ratio\", \"enrichment_factor\"])\n",
    "frag_df.to_csv(ARTIFACTS / \"fragments_enrichment.csv\", index=False)\n",
    "\n",
    "top_frag = frag_df.head(15)\n",
    "if len(top_frag):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(data=top_frag, x=\"fragment\", y=\"odds_ratio\", color=\"#cc6677\")\n",
    "    plt.xticks(rotation=60, ha=\"right\")\n",
    "    plt.title(\"Top enriched Morgan bits in low_MIC\")\n",
    "    plt.tight_layout(); plt.savefig(FIG_DIR / \"fragments_top_enrichment.png\", dpi=220); plt.close()\n",
    "\n",
    "# D) Rule extraction with decision tree on top descriptors + top bits\n",
    "top_desc = uni_df.sort_values(\"mannwhitney_q\").head(min(4, len(uni_df)))[\"descriptor\"].tolist()\n",
    "top_bits = frag_df.head(min(6, len(frag_df)))[\"fragment\"].tolist()\n",
    "rule_features = top_desc + top_bits\n",
    "rule_matrix = pd.concat([train_desc[top_desc], train_fp[top_bits]], axis=1) if len(rule_features) else pd.DataFrame()\n",
    "\n",
    "rules_out = []\n",
    "if len(rule_features):\n",
    "    for depth in [2, 3]:\n",
    "        tree = DecisionTreeClassifier(max_depth=depth, random_state=SEED, min_samples_leaf=3)\n",
    "        tree.fit(rule_matrix, y_cls)\n",
    "\n",
    "        children_left = tree.tree_.children_left\n",
    "        children_right = tree.tree_.children_right\n",
    "        feature = tree.tree_.feature\n",
    "        threshold = tree.tree_.threshold\n",
    "\n",
    "        def walk(node, conds):\n",
    "            if children_left[node] == children_right[node]:\n",
    "                leaf_idx = tree.apply(rule_matrix) == node\n",
    "                support = float(leaf_idx.mean())\n",
    "                if support == 0:\n",
    "                    return\n",
    "                tp = int(np.sum((y_cls == 1) & leaf_idx))\n",
    "                pp = int(np.sum(leaf_idx))\n",
    "                p_act = np.mean(y_cls == 1)\n",
    "                precision = tp / pp if pp else 0.0\n",
    "                recall = tp / max(1, np.sum(y_cls == 1))\n",
    "                lift = (precision / p_act) if p_act > 0 else np.nan\n",
    "                pred = int(np.argmax(tree.tree_.value[node][0]))\n",
    "                rules_out.append({\n",
    "                    \"tree_depth\": depth,\n",
    "                    \"rule\": \" AND \".join(conds) if conds else \"TRUE\",\n",
    "                    \"predicted_class\": pred,\n",
    "                    \"support\": support,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"lift\": lift,\n",
    "                })\n",
    "            else:\n",
    "                feat_name = rule_features[feature[node]]\n",
    "                thr = threshold[node]\n",
    "                walk(children_left[node], conds + [f\"{feat_name} <= {thr:.4f}\"])\n",
    "                walk(children_right[node], conds + [f\"{feat_name} > {thr:.4f}\"])\n",
    "\n",
    "        walk(0, [])\n",
    "\n",
    "rules_df = pd.DataFrame(rules_out).sort_values([\"lift\", \"precision\"], ascending=False)\n",
    "rules_df.to_csv(ARTIFACTS / \"rules.csv\", index=False)\n",
    "\n",
    "# E) Experimental holdout evaluation (no fit on exp)\n",
    "exp_checks = []\n",
    "selected_windows = descriptor_windows.head(min(3, len(descriptor_windows)))\n",
    "for idx, row in exp_desc.iterrows() if len(exp_desc) else []:\n",
    "    w_hits = 0\n",
    "    for _, w in selected_windows.iterrows():\n",
    "        op = w[\"rule\"]\n",
    "        thr = float(w[\"threshold\"])\n",
    "        val = row[w[\"descriptor\"]]\n",
    "        ok = (val <= thr) if op == \"<=\" else (val >= thr)\n",
    "        w_hits += int(ok)\n",
    "\n",
    "    enriched_bits = int(exp_fp.loc[idx, top_bits].sum()) if len(top_bits) else 0\n",
    "    triggered_rules = 0\n",
    "    if len(rules_df) and len(rule_features):\n",
    "        one = pd.concat([exp_desc.loc[[idx], top_desc], exp_fp.loc[[idx], top_bits]], axis=1)\n",
    "        for _, rr in rules_df.iterrows():\n",
    "            conds = rr[\"rule\"].split(\" AND \") if rr[\"rule\"] != \"TRUE\" else []\n",
    "            passed = True\n",
    "            for cond in conds:\n",
    "                if \" <= \" in cond:\n",
    "                    f, t = cond.split(\" <= \")\n",
    "                    passed &= float(one.iloc[0][f]) <= float(t)\n",
    "                elif \" > \" in cond:\n",
    "                    f, t = cond.split(\" > \")\n",
    "                    passed &= float(one.iloc[0][f]) > float(t)\n",
    "            if passed:\n",
    "                triggered_rules += 1\n",
    "\n",
    "    exp_checks.append({\n",
    "        \"smiles\": row[\"smiles\"],\n",
    "        \"MIC\": row[\"MIC\"],\n",
    "        \"pMIC\": row[\"pMIC\"],\n",
    "        \"descriptor_window_hits\": w_hits,\n",
    "        \"enriched_fragment_hits\": enriched_bits,\n",
    "        \"triggered_rules\": triggered_rules,\n",
    "    })\n",
    "\n",
    "exp_checks_df = pd.DataFrame(exp_checks)\n",
    "exp_checks_df.to_csv(ARTIFACTS / \"experimental_rule_check.csv\", index=False)\n",
    "\n",
    "if len(exp_checks_df):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(exp_checks_df[\"descriptor_window_hits\"], exp_checks_df[\"enriched_fragment_hits\"], alpha=0.8)\n",
    "    plt.xlabel(\"Descriptor windows hit\")\n",
    "    plt.ylabel(\"Enriched fragments present\")\n",
    "    plt.title(\"Experimental holdout summary\")\n",
    "    plt.tight_layout(); plt.savefig(FIG_DIR / \"experimental_summary.png\", dpi=220); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(exp_checks_df[\"pMIC\"], exp_checks_df[\"triggered_rules\"], alpha=0.8)\n",
    "    plt.xlabel(\"Observed pMIC\")\n",
    "    plt.ylabel(\"Triggered rules\")\n",
    "    plt.title(\"Experimental parity-style summary\")\n",
    "    plt.tight_layout(); plt.savefig(FIG_DIR / \"experimental_parity.png\", dpi=220); plt.close()\n",
    "\n",
    "# F) y-randomization for classification (train only)\n",
    "def cv_auc_for_labels(labels):\n",
    "    model = clone(models[\"LogReg_L2\"])\n",
    "    aucs = []\n",
    "    for tr, te in cv.split(X_desc, labels, groups):\n",
    "        model.fit(X_desc.iloc[tr], labels[tr])\n",
    "        prob = model.predict_proba(X_desc.iloc[te])[:, 1]\n",
    "        aucs.append(roc_auc_score(labels[te], prob))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "real_auc = cv_auc_for_labels(y_cls)\n",
    "yrand_rows = []\n",
    "for i in range(config[\"yrandomization_n\"]):\n",
    "    ys = rng.permutation(y_cls)\n",
    "    yrand_rows.append({\"iteration\": i + 1, \"roc_auc\": cv_auc_for_labels(ys), \"label\": \"scrambled\"})\n",
    "yrand_rows.append({\"iteration\": 0, \"roc_auc\": real_auc, \"label\": \"real\"})\n",
    "\n",
    "yrand_df = pd.DataFrame(yrand_rows)\n",
    "yrand_df.to_csv(ARTIFACTS / \"y_random_classification.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(yrand_df.loc[yrand_df[\"label\"] == \"scrambled\", \"roc_auc\"], bins=15, alpha=0.7, label=\"scrambled\")\n",
    "plt.axvline(real_auc, color=\"red\", lw=2, label=f\"real={real_auc:.3f}\")\n",
    "plt.xlabel(\"ROC-AUC\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"y-randomization (classification, GroupKFold)\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"y_random_classification.png\", dpi=220); plt.close()\n",
    "\n",
    "logger.info(\"Inverse QSAR activity-conditioned pipeline completed\")\n",
    "print(\"Artifacts generated in:\", ARTIFACTS)\n",
    "print(\"- descriptor_correlations.csv\")\n",
    "print(\"- univariate_statistics.csv\")\n",
    "print(\"- descriptor_windows.csv\")\n",
    "print(\"- classification_metrics.csv\")\n",
    "print(\"- fragments_enrichment.csv\")\n",
    "print(\"- rules.csv\")\n",
    "print(\"- experimental_rule_check.csv\")\n",
    "print(\"- y_random_classification.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Формула Q² (документация)\n\nИспользуется только out-of-fold предсказание на `train_lit`:\n\n\\[\nQ^2 = 1 - \frac{PRESS}{TSS},\\quad PRESS = \\sum_i (y_i - \\hat y_{i,OOF})^2,\\quad TSS = \\sum_i (y_i - \bar y_{train\\_lit})^2.\n\\]\n\nВнешний `experimental_holdout` никогда не участвует в обучении/CV.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ограничения и риски\n\n- **Source bias**: литература и эксперимент могут иметь систематические различия условий.\n- **Small sample size**: нестабильность оценок и широкие доверительные интервалы.\n- **Experimental variability**: шум MIC измерений влияет на верхний потолок качества.\n- **Scaffold-empty issue**: для ациклических структур Murcko scaffold может быть пустой (`ACYCLIC`), что ухудшает групповое разбиение.\n- Рекомендации: расширить датасет, делать group split по источникам/сериям, добавить uncertainty и калибровку доверия.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Как отвечать на критику «модель не работает»\n\nЧестная формулировка для защиты:\n\n> Жёсткий scaffold-test действительно снижает метрики — это ожидаемо для реалистичной валидации. Именно поэтому работа не опирается только на одно число R²: добавлены статистические SAR-тесты, y-randomization и Applicability Domain (Williams plot). Итог: модель применима для ранжирования **внутри домена применимости**, а не как универсальный предиктор для любых структур.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Checklist выполнения задач\n\n- [x] Environment & logging + фиксированный seed + `artifacts/run.log`.\n- [x] Без pip-установки RDKit в ноутбуке; безопасная остановка при отсутствии зависимостей.\n- [x] Явный split: `train_lit=0..89`, `experimental_holdout=90+`.\n- [x] Data curation: canonicalization, invalid removal, largest organic fragment, duplicate policy, `curation_report.csv`.\n- [x] CV только на train_lit (GroupKFold по Murcko), OOF-метрики и Q².\n- [x] External validation: experimental_holdout (если есть) + scaffold external test.\n- [x] Statistical SAR evidence (Spearman/Kendall+FDR, Mann–Whitney + Cliff’s delta + bootstrap CI, Fisher design windows).\n- [x] y-randomization (N=50) + CSV + PNG.\n- [x] Applicability Domain: Williams plot + `in_domain` в predictions.\n- [x] Feature importance: Ridge coefficients + RF permutation importance.\n- [x] Overfitting flag: `delta_R2` и `suspicious_overfit`.\n- [x] Артефакты и `validation_bullets_ru.txt`.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experimental rows start at 90\n\nЭкспериментальные строки начинаются с индекса 90. Если в данных меньше 90 строк, `experimental_holdout` пустой, и выполняется только scaffold external test внутри `train_lit`.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}