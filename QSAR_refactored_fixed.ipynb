{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# QSAR pipeline: reproducible exploratory SAR + ranking QSAR within applicability domain\n\nЭтот ноутбук строит воспроизводимый конвейер с честным разделением данных: `train_lit` (строки 0–89) и `experimental_holdout` (строки 90+). Если внешняя предсказательная сила слабая, вывод остаётся валидным за счёт статистического SAR-блока, y-randomization и Applicability Domain."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Environment & logging\nfrom pathlib import Path\nimport json\nimport logging\nimport random\nimport sys\nimport warnings\n\nSEED = 42\nrandom.seed(SEED)\n\nROOT = Path.cwd()\nDATA_PATH = ROOT / \"potok.csv\"\nARTIFACTS = ROOT / \"artifacts\"\nFIG_DIR = ARTIFACTS / \"figures\"\nARTIFACTS.mkdir(parents=True, exist_ok=True)\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\nlog_path = ARTIFACTS / \"run.log\"\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    handlers=[logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(\"qsar\")\nlogger.info(\"Run started\")\n\nconfig = {\n    \"seed\": SEED,\n    \"data_path\": str(DATA_PATH),\n    \"split_strategy\": \"fixed rows: train_lit=0..89, experimental_holdout=90+; GroupKFold by Murcko scaffold\",\n    \"duplicate_policy\": \"median\",\n    \"active_threshold_pMIC\": 1.0,\n    \"rf_params\": {\"n_estimators\": 500, \"max_depth\": None, \"min_samples_leaf\": 1, \"random_state\": SEED, \"n_jobs\": -1},\n    \"fp_params\": {\"radius\": 2, \"nBits\": 2048},\n    \"yrandomization_n\": 50,\n}\n(ARTIFACTS / \"run_config.json\").write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding=\"utf-8\")\nlogger.info(\"Saved run_config.json\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Dependency check (safe stop if RDKit is unavailable)\ntry:\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from scipy import stats\n\n    from sklearn.base import clone\n    from sklearn.compose import ColumnTransformer\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n    from sklearn.impute import SimpleImputer\n    from sklearn.inspection import permutation_importance\n    from sklearn.linear_model import Ridge\n    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n    from sklearn.model_selection import GroupKFold\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n\n    from rdkit import Chem\n    from rdkit.Chem import Descriptors, AllChem\n    from rdkit.Chem.Scaffolds import MurckoScaffold\n    from rdkit.Chem.MolStandardize import rdMolStandardize\nexcept Exception as e:\n    msg = (\n        \"RDKit и/или научный стек недоступны. Нужна локальная среда с RDKit, numpy, pandas, scipy, scikit-learn, matplotlib. \"\n        \"В ноутбуке pip-установка не выполняется по правилам воспроизводимости.\"\n    )\n    logger.error(msg)\n    logger.error(f\"Import error: {e}\")\n    raise SystemExit(msg)\n\nnp.random.seed(SEED)\nwarnings.filterwarnings(\"ignore\")\nlogger.info(\"Dependencies imported successfully\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data loading and fixed split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nif not DATA_PATH.exists():\n    logger.error(f\"Missing data file: {DATA_PATH}\")\n    raise FileNotFoundError(DATA_PATH)\n\nraw = pd.read_csv(DATA_PATH)\nraw.columns = [c.strip() for c in raw.columns]\nlogger.info(f\"Raw rows: {len(raw)}\")\n\nsmiles_col = next((c for c in raw.columns if c.lower() in [\"smiles\", \"smile\", \"canonical_smiles\"]), None)\nactivity_col = next((c for c in raw.columns if c.lower() in [\"pmic\", \"activity\", \"mic\", \"y\", \"target\"]), None)\nif smiles_col is None or activity_col is None:\n    raise ValueError(\"Expected columns for SMILES and activity (e.g., smiles + pMIC/activity/MIC)\")\n\ndf = raw[[smiles_col, activity_col]].copy().rename(columns={smiles_col: \"smiles\", activity_col: \"activity_raw\"})\ndf[\"row_id\"] = np.arange(len(df))\n\n# If activity looks like MIC>0, convert to pMIC = -log10(MIC [mg/mL])\nif (df[\"activity_raw\"] > 0).all() and (df[\"activity_raw\"].max() > 5):\n    df[\"pMIC\"] = -np.log10(df[\"activity_raw\"].astype(float))\nelse:\n    df[\"pMIC\"] = df[\"activity_raw\"].astype(float)\n\ntrain_lit = df.iloc[:90].copy()\nexperimental_holdout = df.iloc[90:].copy() if len(df) >= 90 else df.iloc[0:0].copy()\n\nif len(df) < 90:\n    logger.warning(\"Dataset has < 90 rows; experimental_holdout is empty. Using only scaffold external test within train_lit.\")\nelse:\n    logger.info(f\"Split sizes | train_lit={len(train_lit)} | experimental_holdout={len(experimental_holdout)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data curation (mandatory)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nlfc = rdMolStandardize.LargestFragmentChooser()\n\ndef standardize_smiles(smi):\n    mol = Chem.MolFromSmiles(str(smi))\n    if mol is None:\n        return None, \"invalid_smiles\"\n    largest = lfc.choose(mol)\n    if largest is None:\n        return None, \"no_largest_fragment\"\n    can = Chem.MolToSmiles(largest, canonical=True)\n    return can, \"ok\"\n\n\ndef curate_block(block, block_name, duplicate_policy=\"median\"):\n    report_rows = []\n    rows = []\n    for _, r in block.iterrows():\n        std_smi, status = standardize_smiles(r[\"smiles\"])\n        action = \"keep\" if status == \"ok\" else \"drop\"\n        report_rows.append({\n            \"block\": block_name,\n            \"row_id\": int(r[\"row_id\"]),\n            \"original_smiles\": r[\"smiles\"],\n            \"standardized_smiles\": std_smi,\n            \"activity\": float(r[\"pMIC\"]),\n            \"action\": action,\n            \"reason\": status,\n        })\n        if status == \"ok\":\n            rows.append({\"row_id\": int(r[\"row_id\"]), \"smiles\": std_smi, \"pMIC\": float(r[\"pMIC\"])})\n\n    clean = pd.DataFrame(rows)\n    if clean.empty:\n        return clean, pd.DataFrame(report_rows)\n\n    grp = clean.groupby(\"smiles\")[\"pMIC\"]\n    agg_rows = []\n    for smi, vals in grp:\n        vals = vals.values\n        if len(vals) == 1:\n            agg_rows.append({\"smiles\": smi, \"pMIC\": float(vals[0])})\n            continue\n        conflict = np.ptp(vals) > 1e-12\n        if duplicate_policy == \"median\":\n            agg_rows.append({\"smiles\": smi, \"pMIC\": float(np.median(vals))})\n            reason = \"duplicate_median\"\n        elif duplicate_policy == \"mean\":\n            agg_rows.append({\"smiles\": smi, \"pMIC\": float(np.mean(vals))})\n            reason = \"duplicate_mean\"\n        elif duplicate_policy == \"drop_conflicts\":\n            if conflict:\n                reason = \"duplicate_conflict_drop\"\n            else:\n                agg_rows.append({\"smiles\": smi, \"pMIC\": float(vals[0])})\n                reason = \"duplicate_identical_keep\"\n        else:\n            raise ValueError(\"duplicate_policy must be median|mean|drop_conflicts\")\n\n        mask = clean[\"smiles\"] == smi\n        idxs = clean.loc[mask, \"row_id\"].tolist()\n        for rid in idxs:\n            report_rows.append({\n                \"block\": block_name,\n                \"row_id\": int(rid),\n                \"original_smiles\": None,\n                \"standardized_smiles\": smi,\n                \"activity\": None,\n                \"action\": \"duplicate_resolution\",\n                \"reason\": reason,\n            })\n\n    curated = pd.DataFrame(agg_rows).drop_duplicates(\"smiles\").reset_index(drop=True)\n    curated[\"block\"] = block_name\n    return curated, pd.DataFrame(report_rows)\n\ntrain_cur, rep_train = curate_block(train_lit, \"train_lit\", duplicate_policy=config[\"duplicate_policy\"])\nexp_cur, rep_exp = curate_block(experimental_holdout, \"experimental_holdout\", duplicate_policy=config[\"duplicate_policy\"])\ncuration_report = pd.concat([rep_train, rep_exp], ignore_index=True)\ncuration_report.to_csv(ARTIFACTS / \"curation_report.csv\", index=False)\n\nassert train_cur[\"smiles\"].isna().sum() == 0, \"NaN in curated smiles\"\nassert np.isfinite(train_cur[\"pMIC\"]).all(), \"Invalid activity values\"\nlogger.info(f\"After curation | train_lit={len(train_cur)} | experimental_holdout={len(exp_cur)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature engineering, CV, Q², external tests, SAR statistics, y-randomization, AD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Descriptor + fingerprint helpers\n\ndef murcko_scaffold(smi):\n    mol = Chem.MolFromSmiles(smi)\n    if mol is None:\n        return \"INVALID\"\n    scaf = MurckoScaffold.MurckoScaffoldSmiles(mol=mol)\n    return scaf if scaf else \"ACYCLIC\"\n\n\ndef desc_dict(mol):\n    return {\n        \"MolWt\": Descriptors.MolWt(mol),\n        \"MolLogP\": Descriptors.MolLogP(mol),\n        \"TPSA\": Descriptors.TPSA(mol),\n        \"NumHDonors\": Descriptors.NumHDonors(mol),\n        \"NumHAcceptors\": Descriptors.NumHAcceptors(mol),\n        \"NumRotatableBonds\": Descriptors.NumRotatableBonds(mol),\n        \"RingCount\": Descriptors.RingCount(mol),\n        \"HeavyAtomCount\": Descriptors.HeavyAtomCount(mol),\n    }\n\n\ndef featurize(data):\n    rows = []\n    fps = []\n    for _, r in data.iterrows():\n        mol = Chem.MolFromSmiles(r[\"smiles\"])\n        d = desc_dict(mol)\n        d.update({\"smiles\": r[\"smiles\"], \"pMIC\": r[\"pMIC\"], \"scaffold\": murcko_scaffold(r[\"smiles\"])})\n        rows.append(d)\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=config[\"fp_params\"][\"radius\"], nBits=config[\"fp_params\"][\"nBits\"])\n        fps.append(np.array(fp, dtype=int))\n    desc = pd.DataFrame(rows)\n    fp_df = pd.DataFrame(fps, columns=[f\"FP_{i}\" for i in range(config[\"fp_params\"][\"nBits\"])])\n    return desc, fp_df\n\ntrain_desc, train_fp = featurize(train_cur)\nexp_desc, exp_fp = featurize(exp_cur) if len(exp_cur) else (pd.DataFrame(), pd.DataFrame())\n\ndesc_cols = [c for c in train_desc.columns if c not in [\"smiles\", \"pMIC\", \"scaffold\"]]\nX_desc = train_desc[desc_cols].copy()\ny = train_desc[\"pMIC\"].values\ngroups = train_desc[\"scaffold\"].values\n\nbaseline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n    (\"var\", VarianceThreshold()),\n    (\"select\", SelectKBest(score_func=f_regression, k=min(8, len(desc_cols)))),\n    (\"model\", Ridge(alpha=1.0)),\n])\n\nstrong = Pipeline([\n    (\"model\", RandomForestRegressor(**config[\"rf_params\"]))\n])\n\nX_fp = train_fp.copy()\n\ndef oof_predict(model, X, y, groups):\n    cv = GroupKFold(n_splits=min(5, len(np.unique(groups))))\n    oof = np.zeros_like(y, dtype=float)\n    for tr, te in cv.split(X, y, groups):\n        m = clone(model)\n        m.fit(X.iloc[tr], y[tr])\n        oof[te] = m.predict(X.iloc[te])\n    return oof\n\noof_b = oof_predict(baseline, X_desc, y, groups)\noof_s = oof_predict(strong, X_fp, y, groups)\n\ndef reg_metrics(y_true, y_pred, y_ref_mean):\n    press = float(np.sum((y_true - y_pred) ** 2))\n    tss = float(np.sum((y_true - y_ref_mean) ** 2))\n    return {\n        \"R2\": float(r2_score(y_true, y_pred)),\n        \"RMSE\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n        \"Q2\": float(1 - press / tss) if tss > 0 else np.nan,\n    }\n\ncv_b = reg_metrics(y, oof_b, np.mean(y))\ncv_s = reg_metrics(y, oof_s, np.mean(y))\n\n# Additional scaffold external test inside train_lit\nuniq_scaf = train_desc[\"scaffold\"].value_counts().index.tolist()\nheld_scaf = set(uniq_scaf[:max(1, len(uniq_scaf)//4)])\nmask_scaf_test = train_desc[\"scaffold\"].isin(held_scaf).values\n\nXtr_d, Xte_d = X_desc.loc[~mask_scaf_test], X_desc.loc[mask_scaf_test]\nXtr_f, Xte_f = X_fp.loc[~mask_scaf_test], X_fp.loc[mask_scaf_test]\nytr, yte = y[~mask_scaf_test], y[mask_scaf_test]\n\nbaseline_fit = clone(baseline).fit(Xtr_d, ytr)\nstrong_fit = clone(strong).fit(Xtr_f, ytr)\nscaf_pred_b = baseline_fit.predict(Xte_d) if len(Xte_d) else np.array([])\nscaf_pred_s = strong_fit.predict(Xte_f) if len(Xte_f) else np.array([])\nscaf_b = reg_metrics(yte, scaf_pred_b, np.mean(ytr)) if len(Xte_d) else {k: np.nan for k in [\"R2\",\"RMSE\",\"MAE\",\"Q2\"]}\nscaf_s = reg_metrics(yte, scaf_pred_s, np.mean(ytr)) if len(Xte_f) else {k: np.nan for k in [\"R2\",\"RMSE\",\"MAE\",\"Q2\"]}\n\n# Experimental external (if available)\npred_rows = []\nexternal_metrics = []\nif len(exp_desc):\n    ext_Xd, ext_Xf = exp_desc[desc_cols], exp_fp\n    ext_y = exp_desc[\"pMIC\"].values\n    b_all = clone(baseline).fit(X_desc, y)\n    s_all = clone(strong).fit(X_fp, y)\n    ext_pred_b = b_all.predict(ext_Xd)\n    ext_pred_s = s_all.predict(ext_Xf)\n    ext_b = reg_metrics(ext_y, ext_pred_b, np.mean(y))\n    ext_s = reg_metrics(ext_y, ext_pred_s, np.mean(y))\nelse:\n    ext_b = {k: np.nan for k in [\"R2\",\"RMSE\",\"MAE\",\"Q2\"]}\n    ext_s = {k: np.nan for k in [\"R2\",\"RMSE\",\"MAE\",\"Q2\"]}\n\n# Statistical SAR evidence\ncorr_rows = []\nfor c in desc_cols:\n    sp = stats.spearmanr(train_desc[c], train_desc[\"pMIC\"], nan_policy=\"omit\")\n    kd = stats.kendalltau(train_desc[c], train_desc[\"pMIC\"], nan_policy=\"omit\")\n    corr_rows.append({\"descriptor\": c, \"spearman_rho\": sp.correlation, \"spearman_p\": sp.pvalue, \"kendall_tau\": kd.correlation, \"kendall_p\": kd.pvalue})\ncor_df = pd.DataFrame(corr_rows).sort_values(\"spearman_p\")\nm = len(cor_df)\ncor_df[\"fdr_q\"] = np.minimum.accumulate((cor_df[\"spearman_p\"].values * m / (np.arange(1, m+1)))[::-1])[::-1]\ncor_df.to_csv(ARTIFACTS / \"sar_correlations.csv\", index=False)\n\nthr = config[\"active_threshold_pMIC\"]\ntrain_desc[\"is_active\"] = (train_desc[\"pMIC\"] >= thr).astype(int)\n\ndef cliffs_delta(x, y):\n    gt = sum(1 for xi in x for yi in y if xi > yi)\n    lt = sum(1 for xi in x for yi in y if xi < yi)\n    return (gt - lt) / (len(x) * len(y))\n\ngroup_rows = []\nrng = np.random.default_rng(SEED)\nfor c in desc_cols:\n    a = train_desc.loc[train_desc[\"is_active\"]==1, c].values\n    i = train_desc.loc[train_desc[\"is_active\"]==0, c].values\n    if len(a) < 2 or len(i) < 2:\n        continue\n    u = stats.mannwhitneyu(a, i, alternative=\"two-sided\")\n    d = cliffs_delta(a, i)\n    boots = []\n    for _ in range(500):\n        aa = rng.choice(a, size=len(a), replace=True)\n        ii = rng.choice(i, size=len(i), replace=True)\n        boots.append(cliffs_delta(aa, ii))\n    lo, hi = np.percentile(boots, [2.5, 97.5])\n    group_rows.append({\"descriptor\": c, \"mannwhitney_u\": u.statistic, \"p_value\": u.pvalue, \"cliffs_delta\": d, \"cliffs_ci_low\": lo, \"cliffs_ci_high\": hi})\npd.DataFrame(group_rows).to_csv(ARTIFACTS / \"sar_group_tests.csv\", index=False)\n\n# Rational design windows\nwindows = {\n    \"MolLogP_1_4\": lambda d: (d[\"MolLogP\"]>=1) & (d[\"MolLogP\"]<=4),\n    \"HBA_2_8\": lambda d: (d[\"NumHAcceptors\"]>=2) & (d[\"NumHAcceptors\"]<=8),\n    \"TPSA_40_120\": lambda d: (d[\"TPSA\"]>=40) & (d[\"TPSA\"]<=120),\n}\n\nwin_rows = []\nfor wname, f in windows.items():\n    inw = f(train_desc)\n    a = train_desc[\"is_active\"] == 1\n    table = np.array([\n        [np.sum(inw & a), np.sum(inw & ~a)],\n        [np.sum(~inw & a), np.sum(~inw & ~a)],\n    ])\n    odds, p = stats.fisher_exact(table)\n    # Wald CI on log(OR) with Haldane-Anscombe correction\n    t = table.astype(float) + 0.5\n    or_corr = (t[0,0]*t[1,1])/(t[0,1]*t[1,0])\n    se = np.sqrt(np.sum(1/t))\n    lcl = np.exp(np.log(or_corr)-1.96*se)\n    ucl = np.exp(np.log(or_corr)+1.96*se)\n    win_rows.append({\"window\": wname, \"a_in_active\": int(table[0,0]), \"b_in_inactive\": int(table[0,1]), \"c_out_active\": int(table[1,0]), \"d_out_inactive\": int(table[1,1]), \"odds_ratio\": odds, \"fisher_p\": p, \"or_ci_low\": lcl, \"or_ci_high\": ucl})\npd.DataFrame(win_rows).to_csv(ARTIFACTS / \"sar_design_windows.csv\", index=False)\n\n# y-randomization (baseline only)\nyr = []\nfor i in range(config[\"yrandomization_n\"]):\n    ys = rng.permutation(y)\n    ypred = oof_predict(baseline, X_desc, ys, groups)\n    mtr = reg_metrics(ys, ypred, np.mean(ys))\n    mtr[\"iter\"] = i + 1\n    yr.append(mtr)\nyrand_df = pd.DataFrame(yr)\nyrand_df.to_csv(ARTIFACTS / \"y_randomization.csv\", index=False)\n\nplt.figure(figsize=(6,4))\nplt.scatter(yrand_df[\"R2\"], yrand_df[\"Q2\"], alpha=0.7, label=\"y-scrambled\")\nplt.scatter([cv_b[\"R2\"]], [cv_b[\"Q2\"]], color=\"red\", label=\"real model\")\nplt.xlabel(\"R² (OOF)\")\nplt.ylabel(\"Q² (OOF)\")\nplt.title(\"y-randomization sanity check\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"y_randomization_plot.png\", dpi=200)\nplt.close()\n\n# AD / Williams plot on baseline with all train-fitted model\nb_all = clone(baseline).fit(X_desc, y)\nX_proc = b_all[:-1].transform(X_desc)\nX_proc = np.asarray(X_proc)\nXtX_inv = np.linalg.pinv(X_proc.T @ X_proc)\nlev_train = np.sum((X_proc @ XtX_inv) * X_proc, axis=1)\nres_train = y - b_all.predict(X_desc)\nstd_res_train = (res_train - np.mean(res_train)) / (np.std(res_train, ddof=1) + 1e-12)\np = X_proc.shape[1]\nn = X_proc.shape[0]\nh_star = 3*(p+1)/n\n\npred_table = []\nfor i, smi in enumerate(train_desc[\"smiles\"]):\n    pred_table.append({\"set\":\"train_lit\",\"smiles\":smi,\"y_true\":y[i],\"y_pred\":b_all.predict(X_desc.iloc[[i]])[0],\"residual\":res_train[i],\"leverage\":lev_train[i],\"std_res\":std_res_train[i],\"in_domain\": (lev_train[i]<=h_star) and (abs(std_res_train[i])<=3)})\n\n# scaffold test rows\nif len(Xte_d):\n    Xte_proc = b_all[:-1].transform(Xte_d)\n    Xte_proc = np.asarray(Xte_proc)\n    lev_te = np.sum((Xte_proc @ XtX_inv) * Xte_proc, axis=1)\n    pred_te = b_all.predict(Xte_d)\n    res_te = yte - pred_te\n    std_te = (res_te - np.mean(res_train)) / (np.std(res_train, ddof=1)+1e-12)\n    smi_te = train_desc.loc[mask_scaf_test, \"smiles\"].values\n    for i in range(len(pred_te)):\n        pred_table.append({\"set\":\"scaffold_test\",\"smiles\":smi_te[i],\"y_true\":yte[i],\"y_pred\":pred_te[i],\"residual\":res_te[i],\"leverage\":lev_te[i],\"std_res\":std_te[i],\"in_domain\": (lev_te[i]<=h_star) and (abs(std_te[i])<=3)})\n\n# experimental rows\nif len(exp_desc):\n    Xexp_proc = b_all[:-1].transform(exp_desc[desc_cols])\n    Xexp_proc = np.asarray(Xexp_proc)\n    lev_exp = np.sum((Xexp_proc @ XtX_inv) * Xexp_proc, axis=1)\n    pred_exp = b_all.predict(exp_desc[desc_cols])\n    y_exp = exp_desc[\"pMIC\"].values\n    res_exp = y_exp - pred_exp\n    std_exp = (res_exp - np.mean(res_train)) / (np.std(res_train, ddof=1)+1e-12)\n    for i, smi in enumerate(exp_desc[\"smiles\"].values):\n        pred_table.append({\"set\":\"experimental_holdout\",\"smiles\":smi,\"y_true\":y_exp[i],\"y_pred\":pred_exp[i],\"residual\":res_exp[i],\"leverage\":lev_exp[i],\"std_res\":std_exp[i],\"in_domain\": (lev_exp[i]<=h_star) and (abs(std_exp[i])<=3)})\n\npred_df = pd.DataFrame(pred_table)\npred_df.to_csv(ARTIFACTS / \"predictions_test.csv\", index=False)\n\n# Figures for external/scaffold (use best available external set)\nplot_set = \"experimental_holdout\" if (pred_df[\"set\"]==\"experimental_holdout\").any() else \"scaffold_test\"\nplot_df = pred_df[pred_df[\"set\"]==plot_set].copy()\nif len(plot_df):\n    # parity\n    for model_name, yhat in [(\"baseline\", plot_df[\"y_pred\"].values)]:\n        plt.figure(figsize=(5,5))\n        plt.scatter(plot_df[\"y_true\"], yhat, alpha=0.8)\n        lo, hi = min(plot_df[\"y_true\"].min(), yhat.min()), max(plot_df[\"y_true\"].max(), yhat.max())\n        plt.plot([lo,hi],[lo,hi], 'k--')\n        plt.xlabel(\"Observed pMIC = -log10(MIC [mg/mL])\")\n        plt.ylabel(\"Predicted pMIC = -log10(MIC [mg/mL])\")\n        plt.title(f\"Parity plot ({model_name}, {plot_set})\")\n        plt.tight_layout(); plt.savefig(FIG_DIR / f\"parity_{model_name}_{plot_set}.png\", dpi=200); plt.close()\n\n    plt.figure(figsize=(6,4))\n    plt.scatter(plot_df[\"y_pred\"], plot_df[\"residual\"], alpha=0.8)\n    plt.axhline(0, color='k', ls='--')\n    plt.xlabel(\"Predicted pMIC = -log10(MIC [mg/mL])\")\n    plt.ylabel(\"Residual (observed - predicted)\")\n    plt.title(f\"Residuals vs predicted ({plot_set})\")\n    plt.tight_layout(); plt.savefig(FIG_DIR / f\"residuals_vs_pred_{plot_set}.png\", dpi=200); plt.close()\n\n    plt.figure(figsize=(6,4))\n    plt.hist(plot_df[\"residual\"], bins=15)\n    plt.xlabel(\"Residual (observed - predicted)\")\n    plt.ylabel(\"Count\")\n    plt.title(f\"Residual histogram ({plot_set})\")\n    plt.tight_layout(); plt.savefig(FIG_DIR / f\"residual_hist_{plot_set}.png\", dpi=200); plt.close()\n\n# Williams plot\nplt.figure(figsize=(7,5))\nfor s, m in [(\"train_lit\", \"o\"), (\"scaffold_test\", \"s\"), (\"experimental_holdout\", \"^\")]:\n    sub = pred_df[pred_df[\"set\"]==s]\n    if len(sub):\n        plt.scatter(sub[\"leverage\"], sub[\"std_res\"], label=s, marker=m, alpha=0.75)\nplt.axhline(3, color=\"r\", ls=\"--\"); plt.axhline(-3, color=\"r\", ls=\"--\")\nplt.axvline(h_star, color=\"purple\", ls=\"--\", label=f\"h*={h_star:.3f}\")\nplt.xlabel(\"Leverage (h)\")\nplt.ylabel(\"Standardized residual\")\nplt.title(\"Williams plot (baseline Ridge)\")\nplt.legend()\nplt.tight_layout(); plt.savefig(FIG_DIR / \"williams_plot.png\", dpi=220); plt.close()\n\n# Feature importance exports\n# Ridge standardized coefficients (on processed selected features)\nridge_model = b_all.named_steps[\"model\"]\nmask_var = b_all.named_steps[\"var\"].get_support()\ndesc_after_var = np.array(desc_cols)[mask_var]\nmask_sel = b_all.named_steps[\"select\"].get_support()\nselected = desc_after_var[mask_sel]\nridge_imp = pd.DataFrame({\"model\":\"ridge\",\"feature\":selected,\"importance\":ridge_model.coef_})\n\nrf_all = clone(strong).fit(X_fp, y)\nperm = permutation_importance(rf_all, X_fp, y, n_repeats=10, random_state=SEED, n_jobs=-1)\nrf_imp = pd.DataFrame({\"model\":\"rf\",\"feature\":X_fp.columns,\"importance\":perm.importances_mean}).sort_values(\"importance\", ascending=False).head(10)\n\nfeat_imp = pd.concat([ridge_imp.assign(abs_importance=lambda d: d[\"importance\"].abs()).sort_values(\"abs_importance\", ascending=False).head(10), rf_imp], ignore_index=True)\nfeat_imp.to_csv(ARTIFACTS / \"feature_importance.csv\", index=False)\n\n# Metrics summary + overfitting flag\nrows = []\nfor model_name, cvm, sm, em in [(\"baseline_ridge\", cv_b, scaf_b, ext_b), (\"strong_rf\", cv_s, scaf_s, ext_s)]:\n    r = {\n        \"model\": model_name,\n        \"R2_CV\": cvm[\"R2\"], \"RMSE_CV\": cvm[\"RMSE\"], \"MAE_CV\": cvm[\"MAE\"], \"Q2_CV\": cvm[\"Q2\"],\n        \"R2_scaffold_test\": sm[\"R2\"], \"RMSE_scaffold_test\": sm[\"RMSE\"],\n        \"R2_external\": em[\"R2\"], \"RMSE_external\": em[\"RMSE\"],\n    }\n    r2_test = em[\"R2\"] if np.isfinite(em[\"R2\"]) else sm[\"R2\"]\n    r[\"delta_R2\"] = r[\"R2_CV\"] - r2_test if np.isfinite(r2_test) else np.nan\n    r[\"suspicious_overfit\"] = bool(r[\"delta_R2\"] > 0.2) if np.isfinite(r[\"delta_R2\"]) else False\n    rows.append(r)\nmetrics = pd.DataFrame(rows)\nmetrics.to_csv(ARTIFACTS / \"metrics_summary.csv\", index=False)\n\n# Validation bullets\next_for_bullets = pred_df[pred_df[\"set\"]==(\"experimental_holdout\" if (pred_df[\"set\"]==\"experimental_holdout\").any() else \"scaffold_test\")]\nin_domain_share = float(ext_for_bullets[\"in_domain\"].mean()) if len(ext_for_bullets) else np.nan\nbase_row = metrics.loc[metrics[\"model\"]==\"baseline_ridge\"].iloc[0]\nbullets = [\n    f\"Q²(CV, baseline Ridge): {base_row['Q2_CV']:.3f}\",\n    f\"RMSE(CV, baseline Ridge): {base_row['RMSE_CV']:.3f}\",\n    f\"RMSE(external/scaffold): {base_row['RMSE_external'] if np.isfinite(base_row['RMSE_external']) else base_row['RMSE_scaffold_test']:.3f}\",\n    f\"Доля external в AD (Williams): {in_domain_share:.2%}\" if np.isfinite(in_domain_share) else \"Доля external в AD: n/a\",\n    \"Цель: статистически обоснованный SAR и feature-selection; QSAR используется как ранжирование только в пределах applicability domain.\",\n    \"Даже при падении метрик на жестком scaffold-test это ожидаемо: добавлены SAR-тесты, AD и y-randomization как контроль валидности.\",\n]\n\nspeech = \"\\n\".join([\n    \"Сценарий 30–45 сек:\",\n    \"1) Мы решаем задачу exploratory SAR: какие параметры реально связаны с pMIC.\",\n    \"2) QSAR-модель применяем как инструмент ранжирования кандидатов только внутри applicability domain.\",\n    \"3) Вклад дескрипторов подтверждён Spearman/Kendall, Mann–Whitney+Cliff’s delta и Fisher enrichment по дизайн-окнам.\",\n    \"4) Ограничения честно учитываются: маленькая выборка, source bias и жесткий scaffold split; план — расширение данных и uncertainty-aware modeling.\",\n])\n(ARTIFACTS / \"validation_bullets_ru.txt\").write_text(\"\\n\".join([\"- \"+b for b in bullets]) + \"\\n\\n\" + speech, encoding=\"utf-8\")\n\nlogger.info(\"Pipeline completed\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Формула Q² (документация)\n\nИспользуется только out-of-fold предсказание на `train_lit`:\n\n\\[\nQ^2 = 1 - \frac{PRESS}{TSS},\\quad PRESS = \\sum_i (y_i - \\hat y_{i,OOF})^2,\\quad TSS = \\sum_i (y_i - \bar y_{train\\_lit})^2.\n\\]\n\nВнешний `experimental_holdout` никогда не участвует в обучении/CV.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ограничения и риски\n\n- **Source bias**: литература и эксперимент могут иметь систематические различия условий.\n- **Small sample size**: нестабильность оценок и широкие доверительные интервалы.\n- **Experimental variability**: шум MIC измерений влияет на верхний потолок качества.\n- **Scaffold-empty issue**: для ациклических структур Murcko scaffold может быть пустой (`ACYCLIC`), что ухудшает групповое разбиение.\n- Рекомендации: расширить датасет, делать group split по источникам/сериям, добавить uncertainty и калибровку доверия.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Как отвечать на критику «модель не работает»\n\nЧестная формулировка для защиты:\n\n> Жёсткий scaffold-test действительно снижает метрики — это ожидаемо для реалистичной валидации. Именно поэтому работа не опирается только на одно число R²: добавлены статистические SAR-тесты, y-randomization и Applicability Domain (Williams plot). Итог: модель применима для ранжирования **внутри домена применимости**, а не как универсальный предиктор для любых структур.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Checklist выполнения задач\n\n- [x] Environment & logging + фиксированный seed + `artifacts/run.log`.\n- [x] Без pip-установки RDKit в ноутбуке; безопасная остановка при отсутствии зависимостей.\n- [x] Явный split: `train_lit=0..89`, `experimental_holdout=90+`.\n- [x] Data curation: canonicalization, invalid removal, largest organic fragment, duplicate policy, `curation_report.csv`.\n- [x] CV только на train_lit (GroupKFold по Murcko), OOF-метрики и Q².\n- [x] External validation: experimental_holdout (если есть) + scaffold external test.\n- [x] Statistical SAR evidence (Spearman/Kendall+FDR, Mann–Whitney + Cliff’s delta + bootstrap CI, Fisher design windows).\n- [x] y-randomization (N=50) + CSV + PNG.\n- [x] Applicability Domain: Williams plot + `in_domain` в predictions.\n- [x] Feature importance: Ridge coefficients + RF permutation importance.\n- [x] Overfitting flag: `delta_R2` и `suspicious_overfit`.\n- [x] Артефакты и `validation_bullets_ru.txt`.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experimental rows start at 90\n\nЭкспериментальные строки начинаются с индекса 90. Если в данных меньше 90 строк, `experimental_holdout` пустой, и выполняется только scaffold external test внутри `train_lit`.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}