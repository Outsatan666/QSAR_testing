{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Inverse QSAR: activity-conditioned structure-from-activity framework\n\nЦель: не универсальное предсказание MIC, а выявление структурных/физико-химических детерминант низкого MIC (высокого pMIC) на `train_lit` (строки 0–89), с отдельной проверкой правил на `experimental_holdout` (строки 90+)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Environment, reproducibility, logging\nfrom pathlib import Path\nimport json\nimport logging\nimport random\nimport sys\nimport warnings\n\nSEED = 42\nrandom.seed(SEED)\n\nROOT = Path.cwd()\nDATA_PATH = ROOT / \"potok.csv\"\nART = ROOT / \"artifacts\"\nFIG = ART / \"figures\"\nART.mkdir(parents=True, exist_ok=True)\nFIG.mkdir(parents=True, exist_ok=True)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n    handlers=[logging.FileHandler(ART / \"run.log\", mode=\"w\", encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n)\nlogger = logging.getLogger(\"inverse_qsar\")\n\nconfig = {\n    \"seed\": SEED,\n    \"task\": \"inverse_qsar_structure_from_activity\",\n    \"split\": \"rows 0-89 -> train_lit; rows 90+ -> experimental_holdout\",\n    \"low_mic_method\": \"Q25 of MIC on train_lit\",\n    \"cv\": \"GroupKFold by Murcko scaffold on train_lit only\",\n    \"morgan\": {\"radius\": 2, \"nBits\": 1024},\n    \"y_random_n\": 50,\n}\n(ART / \"run_config.json\").write_text(json.dumps(config, indent=2, ensure_ascii=False), encoding=\"utf-8\")\nlogger.info(\"Config saved\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Dependency check (safe stop, no pip install inside notebook)\ntry:\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from scipy import stats\n    from sklearn.base import clone\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_selection import mutual_info_regression\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import (\n        roc_auc_score, average_precision_score, balanced_accuracy_score,\n        precision_score, recall_score, roc_curve, precision_recall_curve\n    )\n    from sklearn.model_selection import GroupKFold\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.tree import DecisionTreeClassifier\n\n    from rdkit import Chem\n    from rdkit.Chem import AllChem, Descriptors\n    from rdkit.Chem.MolStandardize import rdMolStandardize\n    from rdkit.Chem.Scaffolds import MurckoScaffold\nexcept Exception as e:\n    msg = (\n        \"Требуется локальная среда с RDKit + numpy/pandas/scipy/scikit-learn/matplotlib. \"\n        \"Установка через pip в ноутбуке отключена для воспроизводимости.\"\n    )\n    logger.error(msg)\n    logger.error(f\"Import error: {e}\")\n    raise SystemExit(msg)\n\nnp.random.seed(SEED)\nwarnings.filterwarnings(\"ignore\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data split and target engineering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nif not DATA_PATH.exists():\n    raise FileNotFoundError(DATA_PATH)\n\nraw = pd.read_csv(DATA_PATH)\nraw.columns = [c.strip() for c in raw.columns]\n\nsmiles_col = next((c for c in raw.columns if c.lower() in [\"smiles\",\"smile\",\"canonical_smiles\"]), None)\nmic_col = next((c for c in raw.columns if c.lower() in [\"mic\",\"activity\",\"target\",\"y\"]), None)\nif smiles_col is None or mic_col is None:\n    raise ValueError(\"Need SMILES and MIC-like column\")\n\ndf = raw[[smiles_col, mic_col]].rename(columns={smiles_col: \"smiles\", mic_col: \"MIC\"}).copy()\ndf[\"row_id\"] = np.arange(len(df))\ndf[\"MIC\"] = pd.to_numeric(df[\"MIC\"], errors=\"coerce\")\n\ntrain_lit = df.iloc[:90].copy()\nexperimental_holdout = df.iloc[90:].copy() if len(df) >= 90 else df.iloc[0:0].copy()\nif len(df) < 90:\n    logger.warning(\"<90 rows: experimental_holdout is empty\")\n\n# Target engineering only from train_lit\nq25_mic = float(train_lit[\"MIC\"].quantile(0.25))\ntrain_lit[\"pMIC\"] = -np.log10(train_lit[\"MIC\"])\ntrain_lit[\"low_MIC\"] = (train_lit[\"MIC\"] <= q25_mic).astype(int)\n\nexperimental_holdout[\"pMIC\"] = -np.log10(experimental_holdout[\"MIC\"]) if len(experimental_holdout) else []\nexperimental_holdout[\"low_MIC\"] = (experimental_holdout[\"MIC\"] <= q25_mic).astype(int) if len(experimental_holdout) else []\n\nlogger.info(f\"Rows: total={len(df)}, train_lit={len(train_lit)}, experimental_holdout={len(experimental_holdout)}\")\nlogger.info(f\"low_MIC threshold (Q25 MIC on train_lit) = {q25_mic:.6g}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Curation and feature generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nlfc = rdMolStandardize.LargestFragmentChooser()\n\ndef std_smiles(smi):\n    m = Chem.MolFromSmiles(str(smi))\n    if m is None:\n        return None\n    m = lfc.choose(m)\n    if m is None:\n        return None\n    return Chem.MolToSmiles(m, canonical=True)\n\n\ndef descs(m):\n    return {\n        \"MolWt\": Descriptors.MolWt(m),\n        \"MolLogP\": Descriptors.MolLogP(m),\n        \"TPSA\": Descriptors.TPSA(m),\n        \"HBD\": Descriptors.NumHDonors(m),\n        \"HBA\": Descriptors.NumHAcceptors(m),\n        \"RotB\": Descriptors.NumRotatableBonds(m),\n        \"RingCount\": Descriptors.RingCount(m),\n        \"HeavyAtomCount\": Descriptors.HeavyAtomCount(m),\n    }\n\n\ndef scaffold(smi):\n    m = Chem.MolFromSmiles(smi)\n    s = MurckoScaffold.MurckoScaffoldSmiles(mol=m)\n    return s if s else \"ACYCLIC\"\n\n\ndef build_features(block, block_name):\n    rows, fps, reports, bit_infos = [], [], [], []\n    for _, r in block.iterrows():\n        s = std_smiles(r[\"smiles\"])\n        if s is None or pd.isna(r[\"MIC\"]) or r[\"MIC\"] <= 0:\n            reports.append({\"block\": block_name, \"row_id\": int(r[\"row_id\"]), \"original_smiles\": r[\"smiles\"], \"standardized_smiles\": s, \"MIC\": r[\"MIC\"], \"action\": \"drop\"})\n            continue\n        m = Chem.MolFromSmiles(s)\n        d = descs(m)\n        d.update({\"smiles\": s, \"MIC\": float(r[\"MIC\"]), \"pMIC\": float(-np.log10(r[\"MIC\"])), \"low_MIC\": int(r[\"MIC\"] <= q25_mic), \"scaffold\": scaffold(s), \"row_id\": int(r[\"row_id\"])})\n        rows.append(d)\n        bi = {}\n        fp = AllChem.GetMorganFingerprintAsBitVect(m, radius=config[\"morgan\"][\"radius\"], nBits=config[\"morgan\"][\"nBits\"], bitInfo=bi)\n        fps.append(np.array(fp, dtype=int))\n        bit_infos.append(bi)\n        reports.append({\"block\": block_name, \"row_id\": int(r[\"row_id\"]), \"original_smiles\": r[\"smiles\"], \"standardized_smiles\": s, \"MIC\": r[\"MIC\"], \"action\": \"keep\"})\n    feat = pd.DataFrame(rows)\n    fp_df = pd.DataFrame(fps, columns=[f\"bit_{i}\" for i in range(config[\"morgan\"][\"nBits\"])]) if len(fps) else pd.DataFrame()\n    rep = pd.DataFrame(reports)\n    return feat, fp_df, rep, bit_infos\n\ntrain_feat, train_fp, rep1, train_bitinfo = build_features(train_lit, \"train_lit\")\nexp_feat, exp_fp, rep2, exp_bitinfo = build_features(experimental_holdout, \"experimental_holdout\")\npd.concat([rep1, rep2], ignore_index=True).to_csv(ART / \"curation_report.csv\", index=False)\n\nnum_desc = [c for c in train_feat.columns if c in [\"MolWt\",\"MolLogP\",\"TPSA\",\"HBD\",\"HBA\",\"RotB\",\"RingCount\",\"HeavyAtomCount\"]]\nassert train_feat[\"smiles\"].isna().sum() == 0\nassert (train_feat[\"MIC\"] > 0).all()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Continuous SAR correlations (train_lit only)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\ncorr_rows = []\nfor c in num_desc:\n    sp = stats.spearmanr(train_feat[c], train_feat[\"pMIC\"], nan_policy=\"omit\")\n    kd = stats.kendalltau(train_feat[c], train_feat[\"pMIC\"], nan_policy=\"omit\")\n    mi = mutual_info_regression(train_feat[[c]], train_feat[\"pMIC\"], random_state=SEED)[0]\n    corr_rows.append({\n        \"descriptor\": c,\n        \"spearman_rho\": sp.correlation,\n        \"spearman_p\": sp.pvalue,\n        \"kendall_tau\": kd.correlation,\n        \"kendall_p\": kd.pvalue,\n        \"mutual_information\": mi,\n    })\ncor = pd.DataFrame(corr_rows).sort_values(\"spearman_p\").reset_index(drop=True)\nm = len(cor)\nrank = np.arange(1, m+1)\ncor[\"fdr_bh_q\"] = np.minimum.accumulate((cor[\"spearman_p\"].values * m / rank)[::-1])[::-1]\ncor.to_csv(ART / \"descriptor_correlations.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Univariate active-vs-inactive statistics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\ndef cliffs_delta(x, y):\n    gt = sum(1 for xi in x for yi in y if xi > yi)\n    lt = sum(1 for xi in x for yi in y if xi < yi)\n    return (gt - lt) / (len(x) * len(y))\n\nrows = []\ny_bin = train_feat[\"low_MIC\"].values\nfor c in num_desc:\n    x = train_feat[c].values\n    x1 = x[y_bin==1]\n    x0 = x[y_bin==0]\n    if len(x1) < 3 or len(x0) < 3:\n        continue\n    u = stats.mannwhitneyu(x1, x0, alternative=\"two-sided\")\n    d = cliffs_delta(x1, x0)\n    auc = roc_auc_score(y_bin, x)\n    fpr, tpr, thr = roc_curve(y_bin, x)\n    j = tpr - fpr\n    idx = int(np.argmax(j))\n    best_thr = float(thr[idx])\n    sens, spec = float(tpr[idx]), float(1-fpr[idx])\n\n    lr = LogisticRegression(penalty='l2', solver='liblinear', random_state=SEED)\n    lr.fit(x.reshape(-1,1), y_bin)\n    beta = float(lr.coef_[0][0])\n    # approximate SE from Hessian\n    p = lr.predict_proba(x.reshape(-1,1))[:,1]\n    W = np.diag(p*(1-p))\n    Xd = np.c_[np.ones(len(x)), x]\n    cov = np.linalg.pinv(Xd.T @ W @ Xd)\n    se = float(np.sqrt(max(cov[1,1], 1e-12)))\n    orr = float(np.exp(beta))\n    lcl = float(np.exp(beta - 1.96*se))\n    ucl = float(np.exp(beta + 1.96*se))\n\n    rows.append({\n        \"descriptor\": c,\n        \"mannwhitney_u\": u.statistic,\n        \"p_value\": u.pvalue,\n        \"cliffs_delta\": d,\n        \"roc_auc\": auc,\n        \"youden_threshold\": best_thr,\n        \"sensitivity\": sens,\n        \"specificity\": spec,\n        \"odds_ratio\": orr,\n        \"or_ci_low\": lcl,\n        \"or_ci_high\": ucl,\n    })\nuni = pd.DataFrame(rows).sort_values(\"p_value\")\nuni.to_csv(ART / \"univariate_statistics.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Scaffold-aware classification of low_MIC"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nX_desc = train_feat[num_desc].copy()\nX_fp = train_fp.copy()\ny = train_feat[\"low_MIC\"].values\ngroups = train_feat[\"scaffold\"].values\n\nmodels = {\n    \"logreg_l2\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", random_state=SEED, max_iter=5000))]),\n    \"logreg_l1\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(penalty=\"l1\", solver=\"liblinear\", random_state=SEED, max_iter=5000))]),\n    \"rf\": RandomForestClassifier(n_estimators=400, random_state=SEED, n_jobs=-1),\n}\n\ncv = GroupKFold(n_splits=min(5, len(np.unique(groups))))\nmetrics_rows = []\nroc_plot = {}\npr_plot = {}\n\nfor name, mdl in models.items():\n    fold_scores = {k: [] for k in [\"roc_auc\",\"pr_auc\",\"balanced_accuracy\",\"precision\",\"recall\"]}\n    y_all, p_all = [], []\n    for tr, te in cv.split(X_desc, y, groups):\n        Xtr = X_fp.iloc[tr] if name==\"rf\" else X_desc.iloc[tr]\n        Xte = X_fp.iloc[te] if name==\"rf\" else X_desc.iloc[te]\n        m = clone(mdl)\n        m.fit(Xtr, y[tr])\n        p = m.predict_proba(Xte)[:,1]\n        pred = (p >= 0.5).astype(int)\n        fold_scores[\"roc_auc\"].append(roc_auc_score(y[te], p))\n        fold_scores[\"pr_auc\"].append(average_precision_score(y[te], p))\n        fold_scores[\"balanced_accuracy\"].append(balanced_accuracy_score(y[te], pred))\n        fold_scores[\"precision\"].append(precision_score(y[te], pred, zero_division=0))\n        fold_scores[\"recall\"].append(recall_score(y[te], pred, zero_division=0))\n        y_all.extend(y[te]); p_all.extend(p)\n\n    y_all = np.array(y_all); p_all = np.array(p_all)\n    fpr, tpr, _ = roc_curve(y_all, p_all)\n    rec, prec, _ = precision_recall_curve(y_all, p_all)\n    roc_plot[name] = (fpr, tpr)\n    pr_plot[name] = (rec, prec)\n\n    metrics_rows.append({\n        \"model\": name,\n        \"roc_auc_mean\": np.mean(fold_scores[\"roc_auc\"]), \"roc_auc_std\": np.std(fold_scores[\"roc_auc\"]),\n        \"pr_auc_mean\": np.mean(fold_scores[\"pr_auc\"]), \"pr_auc_std\": np.std(fold_scores[\"pr_auc\"]),\n        \"balanced_accuracy_mean\": np.mean(fold_scores[\"balanced_accuracy\"]),\n        \"precision_mean\": np.mean(fold_scores[\"precision\"]),\n        \"recall_mean\": np.mean(fold_scores[\"recall\"]),\n    })\n\ncls = pd.DataFrame(metrics_rows).sort_values(\"roc_auc_mean\", ascending=False)\ncls.to_csv(ART / \"classification_metrics.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Fragment enrichment (Morgan bits)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nrows = []\ny = train_feat[\"low_MIC\"].values\nfor bit in train_fp.columns:\n    pres = train_fp[bit].values.astype(int)\n    a = int(np.sum((pres==1) & (y==1)))\n    b = int(np.sum((pres==1) & (y==0)))\n    c = int(np.sum((pres==0) & (y==1)))\n    d = int(np.sum((pres==0) & (y==0)))\n    table = np.array([[a,b],[c,d]])\n    odds, p = stats.fisher_exact(table)\n    # corrected OR and EF\n    t = table + 0.5\n    or_corr = (t[0,0]*t[1,1])/(t[0,1]*t[1,0])\n    p_act = (a + c) / max(a+b+c+d,1)\n    p_bit_act = a / max(a+b,1)\n    ef = p_bit_act / max(p_act,1e-12)\n    rows.append({\"bit\": bit, \"a\":a, \"b\":b, \"c\":c, \"d\":d, \"odds_ratio\":odds, \"odds_ratio_corrected\":or_corr, \"fisher_p\":p, \"enrichment_factor\":ef})\nfrag = pd.DataFrame(rows).sort_values(\"fisher_p\").reset_index(drop=True)\nm = len(frag)\nfrag[\"fdr_bh_q\"] = np.minimum.accumulate((frag[\"fisher_p\"].values * m / (np.arange(1,m+1)))[::-1])[::-1]\nfrag.to_csv(ART / \"fragments_enrichment.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Rule extraction (shallow tree)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\ntop_desc = uni.sort_values(\"p_value\").head(4)[\"descriptor\"].tolist()\ntop_bits = frag.sort_values(\"fisher_p\").head(6)[\"bit\"].tolist()\nrule_features = top_desc + top_bits\nX_rule = pd.concat([train_feat[top_desc], train_fp[top_bits]], axis=1)\ny = train_feat[\"low_MIC\"].values\n\ntree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=SEED)\ntree.fit(X_rule, y)\n\nleaf_id = tree.apply(X_rule)\nrules = []\nfor leaf in np.unique(leaf_id):\n    idx = np.where(leaf_id == leaf)[0]\n    support = len(idx)\n    if support == 0:\n        continue\n    pred_pos = np.mean(y[idx])\n    pred = 1 if pred_pos >= 0.5 else 0\n    if pred != 1:\n        continue\n    precision = np.mean(y[idx]==1)\n    recall = np.sum(y[idx]==1)/max(np.sum(y==1),1)\n    lift = precision / max(np.mean(y==1),1e-12)\n    rules.append({\"rule_id\": int(leaf), \"rule_text\": f\"Leaf {leaf} (tree-derived rule)\", \"precision\": precision, \"recall\": recall, \"support\": support, \"lift\": lift})\n\nrules_df = pd.DataFrame(rules).sort_values([\"lift\",\"precision\"], ascending=False)\nrules_df.to_csv(ART / \"rules.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Check rules/windows on experimental_holdout (90+)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nchecks = []\nif len(exp_feat):\n    # descriptor windows from Youden thresholds and direction by AUC\n    uni_idx = uni.set_index(\"descriptor\")\n    top_desc_use = [d for d in top_desc if d in uni_idx.index]\n    top_bits_use = [b for b in top_bits if b in exp_fp.columns]\n\n    X_exp_rule = pd.concat([exp_feat[top_desc_use], exp_fp[top_bits_use]], axis=1)\n    exp_leaf = tree.apply(X_exp_rule.reindex(columns=X_rule.columns, fill_value=0))\n    rule_hit = np.isin(exp_leaf, rules_df[\"rule_id\"].values) if len(rules_df) else np.zeros(len(exp_feat), dtype=bool)\n\n    for i in range(len(exp_feat)):\n        desc_hits = {}\n        for d in top_desc_use:\n            thr = uni_idx.loc[d, \"youden_threshold\"]\n            direction_high = uni_idx.loc[d, \"roc_auc\"] >= 0.5\n            ok = exp_feat.iloc[i][d] >= thr if direction_high else exp_feat.iloc[i][d] <= thr\n            desc_hits[d] = bool(ok)\n\n        frag_hits = {b: bool(exp_fp.iloc[i][b] == 1) for b in top_bits_use}\n        checks.append({\n            \"smiles\": exp_feat.iloc[i][\"smiles\"],\n            \"row_id\": int(exp_feat.iloc[i][\"row_id\"]),\n            \"matches_descriptor_windows\": all(desc_hits.values()) if len(desc_hits) else False,\n            \"contains_top_fragment\": any(frag_hits.values()) if len(frag_hits) else False,\n            \"matches_rule\": bool(rule_hit[i]),\n            \"low_MIC_observed\": int(exp_feat.iloc[i][\"low_MIC\"]),\n        })\n\nexp_check = pd.DataFrame(checks)\nexp_check.to_csv(ART / \"experimental_rule_check.csv\", index=False)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Y-randomization for classification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nbest_model_name = cls.iloc[0][\"model\"]\nbest_model = models[best_model_name]\nX_best = X_fp if best_model_name==\"rf\" else X_desc\n\nrng = np.random.default_rng(SEED)\nrows = []\nfor i in range(config[\"y_random_n\"]):\n    ys = rng.permutation(train_feat[\"low_MIC\"].values)\n    aucs = []\n    for tr, te in cv.split(X_best, ys, groups):\n        m = clone(best_model)\n        m.fit(X_best.iloc[tr], ys[tr])\n        p = m.predict_proba(X_best.iloc[te])[:,1]\n        aucs.append(roc_auc_score(ys[te], p))\n    rows.append({\"iter\": i+1, \"roc_auc_mean\": float(np.mean(aucs))})\n\nyrnd = pd.DataFrame(rows)\nyrnd.to_csv(ART / \"y_random_classification.csv\", index=False)\n\nplt.figure(figsize=(6,4))\nplt.hist(yrnd[\"roc_auc_mean\"], bins=15, alpha=0.8, label=\"y-scrambled\")\nplt.axvline(float(cls.iloc[0][\"roc_auc_mean\"]), color='r', ls='--', label=f\"real {best_model_name}\")\nplt.xlabel(\"ROC-AUC (GroupKFold mean)\")\nplt.ylabel(\"Count\")\nplt.title(\"Y-randomization for classification\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(ART / \"y_random_plot.png\", dpi=220)\nplt.close()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# ROC curves\nplt.figure(figsize=(6,5))\nfor name, (fpr,tpr) in roc_plot.items():\n    plt.plot(fpr, tpr, label=name)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curves (OOF aggregated)\")\nplt.legend(); plt.tight_layout(); plt.savefig(FIG / \"roc_curves.png\", dpi=220); plt.close()\n\n# PR curves\nplt.figure(figsize=(6,5))\nfor name, (rec,prec) in pr_plot.items():\n    plt.plot(rec, prec, label=name)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"PR curves (OOF aggregated)\")\nplt.legend(); plt.tight_layout(); plt.savefig(FIG / \"pr_curves.png\", dpi=220); plt.close()\n\n# Boxplots for top descriptors\nbox_desc = top_desc if len(top_desc) else num_desc[:4]\nfor d in box_desc:\n    plt.figure(figsize=(5,4))\n    sns.boxplot(data=train_feat, x=\"low_MIC\", y=d)\n    plt.xlabel(\"low_MIC (1=low MIC)\")\n    plt.ylabel(d)\n    plt.title(f\"{d}: low_MIC vs others\")\n    plt.tight_layout(); plt.savefig(FIG / f\"boxplot_{d}.png\", dpi=220); plt.close()\n\n# Top fragments barplot\ntop_frag = frag.sort_values(\"fisher_p\").head(15)\nplt.figure(figsize=(8,4))\nplt.bar(top_frag[\"bit\"], top_frag[\"enrichment_factor\"])\nplt.xticks(rotation=90)\nplt.ylabel(\"Enrichment factor\")\nplt.title(\"Top fragment bits enrichment\")\nplt.tight_layout(); plt.savefig(FIG / \"top_fragments_enrichment.png\", dpi=220); plt.close()\n\n# PCA (train active/inactive + experimental)\ntrain_X = train_feat[num_desc].copy(); train_lbl = np.where(train_feat[\"low_MIC\"]==1, \"train_low_MIC\", \"train_other\")\nif len(exp_feat):\n    exp_X = exp_feat[num_desc].copy(); exp_lbl = np.array([\"experimental\"]*len(exp_feat))\n    X_all = pd.concat([train_X, exp_X], ignore_index=True)\n    lbl = np.concatenate([train_lbl, exp_lbl])\nelse:\n    X_all = train_X; lbl = train_lbl\n\nXp = StandardScaler().fit_transform(X_all)\npcs = PCA(n_components=2, random_state=SEED).fit_transform(Xp)\npca_df = pd.DataFrame({\"PC1\": pcs[:,0], \"PC2\": pcs[:,1], \"set\": lbl})\nplt.figure(figsize=(6,5))\nfor name, g in pca_df.groupby(\"set\"):\n    plt.scatter(g[\"PC1\"], g[\"PC2\"], label=name, alpha=0.8)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA: activity groups and experimental\")\nplt.legend(); plt.tight_layout(); plt.savefig(FIG / \"pca_activity_experimental.png\", dpi=220); plt.close()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Statistical framing and defense outputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nbest = cls.iloc[0]\nsummary = [\n    \"Concept: activity-conditioned structural analysis (inverse QSAR / structure-from-activity).\",\n    f\"low_MIC threshold defined on train_lit only: MIC <= Q25 = {q25_mic:.6g}.\",\n    f\"Best scaffold-aware classifier: {best['model']} with ROC-AUC={best['roc_auc_mean']:.3f}±{best['roc_auc_std']:.3f} and PR-AUC={best['pr_auc_mean']:.3f}±{best['pr_auc_std']:.3f}.\",\n    \"Statistical evidence: descriptor correlations (Spearman/Kendall/MI), univariate tests (Mann-Whitney, Cliff’s delta, OR), fragment enrichment (Fisher + FDR), and rule extraction.\",\n    \"Experimental holdout used only for post-hoc rule/window checks; never for training, thresholding, or CV.\",\n    \"Limitations: small sample size, source bias, experimental variability, scaffold imbalance.\",\n]\n(ART / \"statistical_summary.txt\").write_text(\"\\n\".join(summary), encoding=\"utf-8\")\n\nspeech = [\n    \"Мы используем inverse QSAR: не пытаемся предсказать MIC для всех структур,\",\n    \"а выделяем детерминанты низкого MIC через статистику и scaffold-aware проверку.\",\n    \"Порог low_MIC задан строго по train_lit (нижний квартиль MIC),\",\n    \"после чего подтверждаем вклад дескрипторов и фрагментов независимыми тестами.\",\n    \"Experimental 90+ не участвует в обучении и служит только для проверки правил.\",\n    \"Итог — rule-based framework для дизайна, с честными ограничениями по размеру и смещению данных.\",\n]\n(ART / \"defense_speech.txt\").write_text(\"\\n\".join(speech), encoding=\"utf-8\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Final notes\n\n- `train_lit`: строки 0–89 `potok.csv`.\n- `experimental_holdout`: строки 90+; не участвуют в обучении, выборе порогов и CV.\n- Формулировки о \"predictive QSAR\" намеренно убраны в пользу activity-conditioned statistical framework."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}