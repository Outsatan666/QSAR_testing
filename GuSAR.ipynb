{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# QSAR pipeline for antimicrobial peptidomimetics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experimental holdout (rows 90+)\nНачиная с индекса/пункта 90 находятся новые экспериментальные (синтезированные) соединения.\nОни **не участвуют в обучении и CV**. Используются только как внешний экспериментальный тест (`test_exp`).\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data curation (before features)\n- Canonical SMILES via RDKit (`MolFromSmiles` -> `MolToSmiles(canonical=True)`).\n- Invalid SMILES are removed.\n- Salts/counter-ions are normalized with `LargestFragmentChooser` (keep parent fragment).\n- Duplicates after standardization are resolved by configurable `duplicate_policy` and `tolerance`.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom rdkit import Chem, DataStructs\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\nfrom rdkit.Chem.MolStandardize import rdMolStandardize\nfrom rdkit.ML.Descriptors import MoleculeDescriptors\n\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths and constants\nDATA_PATH = 'potok.csv'  # required columns: smiles, activity\nARTIFACTS_DIR = Path('artifacts')\nARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\nCURATION_REPORT_PATH = ARTIFACTS_DIR / 'curation_report.csv'\nMETRICS_SUMMARY_PATH = ARTIFACTS_DIR / 'metrics_summary.csv'\nPRED_EXP_PATH = ARTIFACTS_DIR / 'predictions_experimental.csv'\n\nLIT_CUTOFF = 90  # rows 0..89 = literature set; rows 90+ = external experimental set\nN_SPLITS_CV = 5\nRANDOM_STATE = 42\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Load raw data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_raw = pd.read_csv(DATA_PATH)\nprint('Raw shape:', df_raw.shape)\nprint('Columns:', list(df_raw.columns))\ndf_raw.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Data curation functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _standardize_smiles(smiles: str, lfc: rdMolStandardize.LargestFragmentChooser):\n    \"\"\"Return standardized canonical parent SMILES, reason, salt-flag.\"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None, 'invalid_smiles', False\n\n    n_frags_before = len(Chem.GetMolFrags(mol))\n    parent = lfc.choose(mol)\n    if parent is None:\n        return None, 'standardization_failed', False\n\n    standardized = Chem.MolToSmiles(parent, canonical=True)\n    salt_norm = n_frags_before > 1\n    reason = 'ok_salt_normalized' if salt_norm else 'ok'\n    return standardized, reason, salt_norm\n\n\ndef curate_data(\n    df: pd.DataFrame,\n    smiles_col: str = 'smiles',\n    activity_col: str = 'activity',\n    duplicate_policy: str = 'median',\n    tolerance: float = 1e-6,\n    report_path: Path = Path('artifacts/curation_report.csv')\n):\n    \"\"\"Unified data curation for QSAR-ready table.\"\"\"\n    allowed = {'median', 'mean', 'drop_conflicts'}\n    if duplicate_policy not in allowed:\n        raise ValueError(f'duplicate_policy must be one of {allowed}')\n\n    required = {smiles_col, activity_col}\n    missing = required - set(df.columns)\n    if missing:\n        raise ValueError(f'Missing required columns: {missing}')\n\n    work = df.copy().reset_index(drop=True)\n    work[activity_col] = pd.to_numeric(work[activity_col], errors='coerce')\n    work = work.dropna(subset=[smiles_col, activity_col]).reset_index(drop=True)\n\n    lfc = rdMolStandardize.LargestFragmentChooser()\n\n    rows = []\n    for row_id, row in work.iterrows():\n        std_smi, reason, salt_norm = _standardize_smiles(row[smiles_col], lfc)\n        rows.append({\n            'row_id': int(row_id),\n            'original_smiles': row[smiles_col],\n            'standardized_smiles': std_smi,\n            'original_activity': float(row[activity_col]),\n            'reason': reason,\n            'salt_normalized': bool(salt_norm),\n        })\n\n    std_df = pd.DataFrame(rows)\n    invalid_mask = std_df['standardized_smiles'].isna()\n\n    report_frames = []\n    if invalid_mask.any():\n        report_frames.append(std_df.loc[invalid_mask, ['original_smiles', 'standardized_smiles', 'reason', 'original_activity']])\n\n    valid_df = std_df.loc[~invalid_mask].copy()\n\n    grouped_rows = []\n    duplicate_groups, duplicate_collapsed, conflicts_found = 0, 0, 0\n\n    for ssmiles, grp in valid_df.groupby('standardized_smiles', sort=False):\n        acts = grp['original_activity'].to_numpy()\n        n = len(grp)\n        if n == 1:\n            grouped_rows.append({\n                'standardized_smiles': ssmiles,\n                'activity': float(acts[0]),\n                'source_row_min': int(grp['row_id'].min())\n            })\n            continue\n\n        duplicate_groups += 1\n        duplicate_collapsed += (n - 1)\n        conflict = (np.max(acts) - np.min(acts)) > tolerance\n        if conflict:\n            conflicts_found += 1\n\n        if (not conflict):\n            final_activity = float(np.median(acts))\n            action = 'duplicates_equal_keep_one'\n            keep_row = True\n        elif duplicate_policy == 'median':\n            final_activity = float(np.median(acts))\n            action = 'duplicates_conflict_aggregated_median'\n            keep_row = True\n        elif duplicate_policy == 'mean':\n            final_activity = float(np.mean(acts))\n            action = 'duplicates_conflict_aggregated_mean'\n            keep_row = True\n        else:\n            final_activity = np.nan\n            action = 'duplicates_conflict_dropped'\n            keep_row = False\n\n        report_frames.append(pd.DataFrame({\n            'original_smiles': grp['original_smiles'].values,\n            'standardized_smiles': grp['standardized_smiles'].values,\n            'reason': [action] * len(grp),\n            'original_activity': grp['original_activity'].values,\n        }))\n\n        if keep_row:\n            grouped_rows.append({\n                'standardized_smiles': ssmiles,\n                'activity': final_activity,\n                'source_row_min': int(grp['row_id'].min())\n            })\n\n    curated_df = pd.DataFrame(grouped_rows).sort_values('source_row_min').reset_index(drop=True)\n\n    report_df = (pd.concat(report_frames, ignore_index=True)\n                 if report_frames else\n                 pd.DataFrame(columns=['original_smiles', 'standardized_smiles', 'reason', 'original_activity']))\n    report_df.to_csv(report_path, index=False)\n\n    summary = {\n        'n_input_rows': int(len(df)),\n        'n_after_numeric_and_notna': int(len(work)),\n        'n_invalid_removed': int(invalid_mask.sum()),\n        'n_salt_normalized': int(std_df['salt_normalized'].sum()),\n        'n_duplicate_groups': int(duplicate_groups),\n        'n_duplicates_collapsed': int(duplicate_collapsed),\n        'n_activity_conflicts': int(conflicts_found),\n        'duplicate_policy': duplicate_policy,\n        'tolerance': tolerance,\n        'n_output_rows': int(len(curated_df)),\n        'report_path': str(report_path),\n    }\n\n    return curated_df, summary, report_df\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "curated_df, curation_summary, curation_report = curate_data(\n    df_raw,\n    smiles_col='smiles',\n    activity_col='activity',\n    duplicate_policy='median',\n    tolerance=1e-6,\n    report_path=CURATION_REPORT_PATH,\n)\n\nprint('=== Data curation summary ===')\nfor k, v in curation_summary.items():\n    print(f'{k}: {v}')\n\nassert curated_df['standardized_smiles'].notna().all(), 'NaN in standardized_smiles after curation.'\nif (curated_df['activity'] <= 0).any():\n    raise ValueError('Found activity <= 0; cannot safely apply log10 transform for target.')\nassert len(curated_df) > 0, 'Curated dataset is empty.'\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Experimental split: train_lit (0–89) vs test_exp (90+)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def split_literature_experimental(df: pd.DataFrame, cutoff: int = 90):\n    \"\"\"Robust split by order: rows 0..cutoff-1 are literature, cutoff+ are experimental.\n\n    If column '№' exists and is aligned with row order, it is used as a helper reference.\n    Otherwise positional index is used.\n    \"\"\"\n    work = df.copy().reset_index(drop=True)\n\n    use_number_column = False\n    if '№' in work.columns:\n        num_col = pd.to_numeric(work['№'], errors='coerce')\n        aligned = num_col.notna().all() and np.allclose(num_col.values, np.arange(len(work)))\n        if aligned:\n            use_number_column = True\n\n    if use_number_column:\n        idx_ref = pd.to_numeric(work['№'])\n    else:\n        idx_ref = pd.Series(np.arange(len(work)))\n\n    train_mask = idx_ref < cutoff\n    test_mask = idx_ref >= cutoff\n\n    train_lit = work.loc[train_mask].reset_index(drop=True)\n    test_exp = work.loc[test_mask].reset_index(drop=True)\n    return train_lit, test_exp\n\ntrain_lit, test_exp = split_literature_experimental(curated_df, cutoff=LIT_CUTOFF)\nprint('train_lit:', train_lit.shape)\nprint('test_exp:', test_exp.shape)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Feature generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "descriptor_names = [\n    'MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors',\n    'NumHeteroatoms', 'NumRotatableBonds', 'TPSA',\n    'NumAromaticRings', 'RingCount', 'FractionCSP3'\n]\ncalc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n\ndef calc_descriptor_frame(mols):\n    rows = [calc.CalcDescriptors(mol) for mol in mols]\n    return pd.DataFrame(rows, columns=descriptor_names)\n\ndef morgan_fp_array(mol, radius=2, n_bits=2048):\n    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n    arr = np.zeros((n_bits,), dtype=np.int8)\n    DataStructs.ConvertToNumpyArray(fp, arr)\n    return arr\n\ndef murcko_scaffold_smiles(mol):\n    return MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_model_table(df):\n    tbl = df.copy()\n    tbl['mol'] = tbl['standardized_smiles'].apply(Chem.MolFromSmiles)\n    tbl = tbl[tbl['mol'].notnull()].reset_index(drop=True)\n    if (tbl['activity'] <= 0).any():\n        raise ValueError('activity must be > 0 for log transform')\n    tbl['y'] = np.log10(tbl['activity'])\n    tbl['scaffold'] = tbl['mol'].apply(murcko_scaffold_smiles)\n    return tbl\n\ntrain_tbl = build_model_table(train_lit)\ntest_exp_tbl = build_model_table(test_exp) if len(test_exp) > 0 else pd.DataFrame(columns=train_tbl.columns)\n\nX_desc_train = calc_descriptor_frame(train_tbl['mol'])\nX_fp_train = np.vstack([morgan_fp_array(m) for m in train_tbl['mol']])\ny_train = train_tbl['y'].to_numpy()\ngroups_train = train_tbl['scaffold'].to_numpy()\n\nif len(test_exp_tbl) > 0:\n    X_desc_exp = calc_descriptor_frame(test_exp_tbl['mol'])\n    X_fp_exp = np.vstack([morgan_fp_array(m) for m in test_exp_tbl['mol']])\n    y_exp = test_exp_tbl['y'].to_numpy()\nelse:\n    X_desc_exp, X_fp_exp, y_exp = None, None, None\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) GroupKFold CV on train_lit + Q²(CV) from OOF predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rmse(y_true, y_pred):\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\ndef q2_cv_from_oof(y_true, y_oof):\n    \"\"\"Q²(CV) = 1 - PRESS/TSS, with PRESS from OOF predictions.\"\"\"\n    y_mean = float(np.mean(y_true))\n    press = float(np.sum((y_true - y_oof) ** 2))\n    tss = float(np.sum((y_true - y_mean) ** 2))\n    if tss == 0:\n        return np.nan\n    return 1.0 - (press / tss)\n\ndef evaluate_cv_groupkfold(model, X, y, groups, n_splits=5):\n    unique_groups = pd.Series(groups).nunique()\n    if unique_groups < 2:\n        raise ValueError('Need at least 2 unique scaffolds for GroupKFold.')\n    n_splits_eff = min(n_splits, unique_groups)\n    cv = GroupKFold(n_splits=n_splits_eff)\n    y_oof = cross_val_predict(model, X, y, cv=cv, groups=groups, n_jobs=-1)\n    return {\n        'R2(CV)': r2_score(y, y_oof),\n        'RMSE(CV)': rmse(y, y_oof),\n        'MAE(CV)': mean_absolute_error(y, y_oof),\n        'Q2(CV)': q2_cv_from_oof(y, y_oof),\n        'n_splits': n_splits_eff,\n    }, y_oof\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "models = {\n    'Descriptors+Ridge': Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler()),\n        ('model', Ridge(alpha=1.0))\n    ]),\n    'MorganFP+RandomForest': Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('model', RandomForestRegressor(\n            n_estimators=500,\n            random_state=RANDOM_STATE,\n            min_samples_leaf=2,\n            n_jobs=-1\n        ))\n    ])\n}\n\nfeature_sets = {\n    'Descriptors+Ridge': (X_desc_train, X_desc_exp),\n    'MorganFP+RandomForest': (X_fp_train, X_fp_exp),\n}\n\nmetrics_rows = []\nexp_pred_rows = []\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional internal scaffold-external split (inside train_lit) for extra reporting\n# It is reported only if feasible by scaffold diversity.\ndef scaffold_external_split(tbl, test_size=0.2, random_state=42):\n    groups = tbl['scaffold'].to_numpy()\n    uniq = pd.Series(groups).nunique()\n    if uniq < 3:\n        return None, None\n    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    idx_train, idx_test = next(splitter.split(tbl, groups=groups))\n    return idx_train, idx_test\n\nscaf_train_idx, scaf_test_idx = scaffold_external_split(train_tbl)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for model_name, model in models.items():\n    X_train_all, X_exp = feature_sets[model_name]\n\n    # 1) CV on train_lit only (GroupKFold by Murcko scaffold)\n    cv_metrics, y_oof = evaluate_cv_groupkfold(\n        model=model,\n        X=X_train_all,\n        y=y_train,\n        groups=groups_train,\n        n_splits=N_SPLITS_CV,\n    )\n    metrics_rows.append({\n        'dataset': 'CV(train_lit)',\n        'model': model_name,\n        'R2': cv_metrics['R2(CV)'],\n        'RMSE': cv_metrics['RMSE(CV)'],\n        'MAE': cv_metrics['MAE(CV)'],\n        'Q2': cv_metrics['Q2(CV)'],\n        'n_samples': len(y_train),\n    })\n\n    # 2) Optional scaffold-external test inside train_lit\n    if scaf_train_idx is not None and scaf_test_idx is not None:\n        model_scaf = model\n        X_scaf_train = X_train_all[scaf_train_idx] if isinstance(X_train_all, np.ndarray) else X_train_all.iloc[scaf_train_idx]\n        X_scaf_test = X_train_all[scaf_test_idx] if isinstance(X_train_all, np.ndarray) else X_train_all.iloc[scaf_test_idx]\n        y_scaf_train, y_scaf_test = y_train[scaf_train_idx], y_train[scaf_test_idx]\n        model_scaf.fit(X_scaf_train, y_scaf_train)\n        y_scaf_pred = model_scaf.predict(X_scaf_test)\n        metrics_rows.append({\n            'dataset': 'external_test_scaffold',\n            'model': model_name,\n            'R2': r2_score(y_scaf_test, y_scaf_pred),\n            'RMSE': rmse(y_scaf_test, y_scaf_pred),\n            'MAE': mean_absolute_error(y_scaf_test, y_scaf_pred),\n            'Q2': np.nan,\n            'n_samples': len(y_scaf_test),\n        })\n\n    # 3) Train on all train_lit, predict external experimental test (rows 90+)\n    model.fit(X_train_all, y_train)\n    if X_exp is not None and len(test_exp_tbl) > 0:\n        y_exp_pred = model.predict(X_exp)\n\n        metrics_rows.append({\n            'dataset': 'external_experimental_test',\n            'model': model_name,\n            'R2': r2_score(y_exp, y_exp_pred),\n            'RMSE': rmse(y_exp, y_exp_pred),\n            'MAE': mean_absolute_error(y_exp, y_exp_pred),\n            'Q2': np.nan,\n            'n_samples': len(y_exp),\n        })\n\n        # save predictions for experimental holdout\n        if model_name == 'MorganFP+RandomForest':\n            pred_table = test_exp_tbl[['standardized_smiles', 'y']].copy()\n            pred_table['y_pred'] = y_exp_pred\n            pred_table['residual'] = pred_table['y'] - pred_table['y_pred']\n            pred_table = pred_table.rename(columns={'standardized_smiles': 'smiles', 'y': 'y_true'})\n            exp_pred_rows.append(pred_table)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Save outputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics_summary = pd.DataFrame(metrics_rows)\nmetrics_summary.to_csv(METRICS_SUMMARY_PATH, index=False)\n\nif exp_pred_rows:\n    predictions_experimental = pd.concat(exp_pred_rows, ignore_index=True)\nelse:\n    predictions_experimental = pd.DataFrame(columns=['smiles', 'y_true', 'y_pred', 'residual'])\n\npredictions_experimental.to_csv(PRED_EXP_PATH, index=False)\n\nprint('Saved:', METRICS_SUMMARY_PATH)\nprint('Saved:', PRED_EXP_PATH)\nprint('Saved:', CURATION_REPORT_PATH)\nmetrics_summary\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
