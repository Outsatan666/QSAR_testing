{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# QSAR notebook for antimicrobial peptidomimetics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data curation (Q1-ready, reproducible)\n- Canonicalize SMILES with RDKit and remove invalid structures.\n- Normalize salts/counter-ions by keeping the largest organic fragment.\n- Resolve duplicates after standardization with configurable policy (`median`, `mean`, `drop_conflicts`).\n- Save a transparent curation audit trail to `artifacts/curation_report.csv`.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional install (uncomment if needed)\n# !pip install rdkit-pypi pandas numpy scikit-learn matplotlib seaborn\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom rdkit import Chem, DataStructs\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem.MolStandardize import rdMolStandardize\nfrom rdkit.ML.Descriptors import MoleculeDescriptors\n\nfrom sklearn.model_selection import RepeatedKFold, cross_validate, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATA_PATH = 'potok.csv'  # required columns: smiles, activity\nARTIFACTS_DIR = Path('artifacts')\nARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\nCURATION_REPORT_PATH = ARTIFACTS_DIR / 'curation_report.csv'\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Load raw data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_raw = pd.read_csv(DATA_PATH)\nprint('Raw shape:', df_raw.shape)\nprint('Columns:', list(df_raw.columns))\ndf_raw.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Unified Data curation block"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _standardize_smiles(smiles: str, lfc: rdMolStandardize.LargestFragmentChooser):\n    \"\"\"Convert raw SMILES to standardized canonical parent SMILES.\n\n    Steps: parse -> keep largest organic fragment -> canonicalize.\n    Returns tuple: (standardized_smiles_or_none, reason, salt_normalized_flag).\n    \"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None, 'invalid_smiles', False\n\n    num_frags_before = len(Chem.GetMolFrags(mol))\n    parent = lfc.choose(mol)\n    if parent is None:\n        return None, 'standardization_failed', False\n\n    std_smiles = Chem.MolToSmiles(parent, canonical=True)\n    salt_normalized = num_frags_before > 1\n    reason = 'ok_salt_normalized' if salt_normalized else 'ok'\n    return std_smiles, reason, salt_normalized\n\n\ndef curate_data(\n    df: pd.DataFrame,\n    smiles_col: str = 'smiles',\n    activity_col: str = 'activity',\n    duplicate_policy: str = 'median',\n    tolerance: float = 1e-6,\n    report_path: Path = Path('artifacts/curation_report.csv')\n):\n    \"\"\"Perform reproducible QSAR data curation.\n\n    Parameters\n    ----------\n    duplicate_policy : {'median', 'mean', 'drop_conflicts'}\n        How to aggregate activity for duplicated standardized structures.\n    tolerance : float\n        Tiny tolerance to treat activity values as effectively equal.\n    report_path : Path\n        Path for row-level curation report.\n\n    Returns\n    -------\n    curated_df : pd.DataFrame\n        Curated table with standardized_smiles and curated activity.\n    summary : dict\n        Summary counts for transparent logging.\n    report_df : pd.DataFrame\n        Detailed report of removed/collapsed records.\n    \"\"\"\n    allowed = {'median', 'mean', 'drop_conflicts'}\n    if duplicate_policy not in allowed:\n        raise ValueError(f'duplicate_policy must be one of {allowed}')\n\n    required = {smiles_col, activity_col}\n    missing = required - set(df.columns)\n    if missing:\n        raise ValueError(f'Missing required columns: {missing}')\n\n    work = df[[smiles_col, activity_col]].copy()\n    work[activity_col] = pd.to_numeric(work[activity_col], errors='coerce')\n    work = work.dropna(subset=[smiles_col, activity_col]).reset_index(drop=True)\n\n    lfc = rdMolStandardize.LargestFragmentChooser()\n\n    rows = []\n    for _, row in work.iterrows():\n        original_smiles = row[smiles_col]\n        act = float(row[activity_col])\n        std_smiles, reason, salt_flag = _standardize_smiles(original_smiles, lfc)\n        rows.append({\n            'original_smiles': original_smiles,\n            'standardized_smiles': std_smiles,\n            'original_activity': act,\n            'reason': reason,\n            'salt_normalized': salt_flag,\n        })\n\n    std_df = pd.DataFrame(rows)\n\n    invalid_mask = std_df['standardized_smiles'].isna()\n    invalid_count = int(invalid_mask.sum())\n    salt_normalized_count = int(std_df['salt_normalized'].sum())\n\n    valid_df = std_df.loc[~invalid_mask].copy()\n\n    report_rows = []\n    if invalid_count > 0:\n        report_rows.append(std_df.loc[invalid_mask, ['original_smiles', 'standardized_smiles', 'reason', 'original_activity']])\n\n    grouped_rows = []\n    duplicate_groups = 0\n    conflicts_found = 0\n    duplicate_collapsed = 0\n\n    for ssmiles, grp in valid_df.groupby('standardized_smiles', sort=False):\n        acts = grp['original_activity'].to_numpy()\n        n = len(grp)\n        if n == 1:\n            grouped_rows.append({'standardized_smiles': ssmiles, 'activity': float(acts[0])})\n            continue\n\n        duplicate_groups += 1\n        duplicate_collapsed += (n - 1)\n        act_range = float(np.max(acts) - np.min(acts))\n        conflict = act_range > tolerance\n        if conflict:\n            conflicts_found += 1\n\n        if (not conflict):\n            final_activity = float(np.median(acts))\n            action = 'duplicates_equal_keep_one'\n            keep_row = True\n        else:\n            if duplicate_policy == 'median':\n                final_activity = float(np.median(acts))\n                action = 'duplicates_conflict_aggregated_median'\n                keep_row = True\n            elif duplicate_policy == 'mean':\n                final_activity = float(np.mean(acts))\n                action = 'duplicates_conflict_aggregated_mean'\n                keep_row = True\n            else:\n                final_activity = np.nan\n                action = 'duplicates_conflict_dropped'\n                keep_row = False\n\n        report_rows.append(pd.DataFrame({\n            'original_smiles': grp['original_smiles'].values,\n            'standardized_smiles': grp['standardized_smiles'].values,\n            'reason': [action] * len(grp),\n            'original_activity': grp['original_activity'].values,\n        }))\n\n        if keep_row:\n            grouped_rows.append({'standardized_smiles': ssmiles, 'activity': final_activity})\n\n    curated_df = pd.DataFrame(grouped_rows)\n\n    if len(report_rows) > 0:\n        report_df = pd.concat(report_rows, ignore_index=True)\n    else:\n        report_df = pd.DataFrame(columns=['original_smiles', 'standardized_smiles', 'reason', 'original_activity'])\n\n    report_df.to_csv(report_path, index=False)\n\n    summary = {\n        'n_input_rows': int(len(df)),\n        'n_after_numeric_and_notna': int(len(work)),\n        'n_invalid_removed': invalid_count,\n        'n_salt_normalized': salt_normalized_count,\n        'n_duplicate_groups': int(duplicate_groups),\n        'n_duplicates_collapsed': int(duplicate_collapsed),\n        'n_activity_conflicts': int(conflicts_found),\n        'duplicate_policy': duplicate_policy,\n        'tolerance': tolerance,\n        'n_output_rows': int(len(curated_df)),\n        'report_path': str(report_path),\n    }\n\n    return curated_df, summary, report_df\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "curated_df, curation_summary, curation_report = curate_data(\n    df_raw,\n    smiles_col='smiles',\n    activity_col='activity',\n    duplicate_policy='median',\n    tolerance=1e-6,\n    report_path=CURATION_REPORT_PATH,\n)\n\nprint('=== Data curation summary ===')\nfor k, v in curation_summary.items():\n    print(f'{k}: {v}')\n\ncurated_df.head()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "assert curated_df['standardized_smiles'].notna().all(), 'NaN in standardized_smiles after curation.'\nif (curated_df['activity'] <= 0).any():\n    raise ValueError('Found activity <= 0; cannot safely apply log10 transform for target.')\nassert len(curated_df) > 0, 'Curated dataset is empty.'\n\nprint('Sanity checks passed.')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Feature generation (uses standardized_smiles only)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_model = curated_df.copy()\ndf_model['mol'] = df_model['standardized_smiles'].apply(Chem.MolFromSmiles)\ndf_model = df_model[df_model['mol'].notnull()].reset_index(drop=True)\ndf_model['logACT'] = np.log10(df_model['activity'])\n\nprint('Modeling shape:', df_model.shape)\ndf_model.head()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "descriptor_names = [\n    'MolWt', 'MolLogP', 'NumHDonors', 'NumHAcceptors',\n    'NumHeteroatoms', 'NumRotatableBonds', 'TPSA',\n    'NumAromaticRings', 'RingCount', 'FractionCSP3'\n]\n\ncalc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n\ndef calc_descriptor_frame(mols):\n    \"\"\"Calculate selected RDKit descriptors for a sequence of molecules.\"\"\"\n    rows = [calc.CalcDescriptors(mol) for mol in mols]\n    return pd.DataFrame(rows, columns=descriptor_names)\n\nX_desc = calc_descriptor_frame(df_model['mol'])\ny = df_model['logACT'].to_numpy()\n\nprint('Descriptor matrix:', X_desc.shape)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def morgan_fp_array(mol, radius=2, n_bits=2048):\n    \"\"\"Convert Morgan bit-vector fingerprint to numpy array.\"\"\"\n    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n    arr = np.zeros((n_bits,), dtype=np.int8)\n    DataStructs.ConvertToNumpyArray(fp, arr)\n    return arr\n\nX_fp = np.vstack([morgan_fp_array(m) for m in df_model['mol']])\nprint('Fingerprint matrix:', X_fp.shape)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Modeling and metrics (kept intact, minimal adaptation)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nscoring = {\n    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n    'RMSE': make_scorer(rmse, greater_is_better=False),\n    'R2': make_scorer(r2_score)\n}\n\ncv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=42)\n\nmodels = {\n    'Ridge_descriptors': Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler()),\n        ('model', Ridge(alpha=1.0))\n    ]),\n    'RF_descriptors': Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('model', RandomForestRegressor(n_estimators=500, random_state=42, min_samples_leaf=2, n_jobs=-1))\n    ]),\n    'Ridge_fingerprint': Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('scaler', StandardScaler(with_mean=False)),\n        ('model', Ridge(alpha=2.0))\n    ])\n}\n\nresults = []\n\ndef summarize_cv(name, model, X, y):\n    out = cross_validate(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n    return {'model': name, 'MAE_mean': -out['test_MAE'].mean(), 'RMSE_mean': -out['test_RMSE'].mean(), 'R2_mean': out['test_R2'].mean()}\n\nresults.append(summarize_cv('Ridge_descriptors', models['Ridge_descriptors'], X_desc, y))\nresults.append(summarize_cv('RF_descriptors', models['RF_descriptors'], X_desc, y))\nresults.append(summarize_cv('Ridge_fingerprint', models['Ridge_fingerprint'], X_fp, y))\n\nb_mae, b_rmse, b_r2 = [], [], []\nfor train_idx, test_idx in cv.split(X_desc):\n    y_train, y_test = y[train_idx], y[test_idx]\n    pred = np.full_like(y_test, y_train.mean())\n    b_mae.append(mean_absolute_error(y_test, pred))\n    b_rmse.append(rmse(y_test, pred))\n    b_r2.append(r2_score(y_test, pred))\n\nresults.append({'model': 'Baseline_mean', 'MAE_mean': float(np.mean(b_mae)), 'RMSE_mean': float(np.mean(b_rmse)), 'R2_mean': float(np.mean(b_r2))})\n\npd.DataFrame(results).sort_values('RMSE_mean')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def tanimoto_to_train(test_mol, train_mols, radius=2, n_bits=2048):\n    fp_test = AllChem.GetMorganFingerprintAsBitVect(test_mol, radius, nBits=n_bits)\n    train_fps = [AllChem.GetMorganFingerprintAsBitVect(m, radius, nBits=n_bits) for m in train_mols]\n    sims = DataStructs.BulkTanimotoSimilarity(fp_test, train_fps)\n    return max(sims) if sims else 0.0\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ntrain_idx, test_idx = next(kf.split(df_model))\n\ntrain_mols = df_model.loc[train_idx, 'mol'].tolist()\ntest_mols = df_model.loc[test_idx, 'mol'].tolist()\n\ntanimoto_max = [tanimoto_to_train(m, train_mols) for m in test_mols]\n\nad_demo = pd.DataFrame({\n    'standardized_smiles': df_model.loc[test_idx, 'standardized_smiles'].values,\n    'logACT': df_model.loc[test_idx, 'logACT'].values,\n    'max_train_tanimoto': tanimoto_max,\n})\n\nplt.figure(figsize=(6, 4))\nsns.histplot(ad_demo['max_train_tanimoto'], bins=15)\nplt.title('Applicability Domain proxy: max train Tanimoto')\nplt.xlabel('Max Tanimoto to train set')\nplt.grid(alpha=0.3)\nplt.show()\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
